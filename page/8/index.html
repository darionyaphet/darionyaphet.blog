<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Page 8 | darion.johannes.yaphet</title>
  <meta name="author" content="Darion Yaphet">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="darion.johannes.yaphet"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="darion.johannes.yaphet" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">darion.johannes.yaphet</a></h1>
  <h2><a href="/">long is the way and hard  that out of Hell leads up to light</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-04-18T16:00:00.000Z"><a href="/2016/04/19/live/game/">2016-04-19</a></time>
      
      
  
    <h1 class="title"><a href="/2016/04/19/live/game/">Game</a></h1>
  

    </header>
    <div class="entry">
      
        <p><a href="http://www.guokr.com/blog/777525/" target="_blank" rel="external">Nim æ¸¸æˆ</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ä¼ ç»Ÿçš„Nimæ¸¸æˆæ˜¯å¯¹ä¸€äº›æ”¾ç½®æˆå †çš„ä¸€å®šæ•°é‡çš„ç¡¬å¸å¼€å§‹çš„ï¼šç¡¬å¸å’Œå †çš„æ•°é‡å–å†³äºä½ ã€‚</span><br><span class="line">æœ‰ä¸¤åç©å®¶ç©è¿™ä¸ªæ¸¸æˆï¼Œå½“è½®åˆ°æŸä½ç©å®¶æ—¶ï¼Œä»–èƒ½ä»æŸä¸€å †é‡Œå–ä»»æ„æ•°é‡çš„ç¡¬å¸ï¼Œä½†æ˜¯è‡³å°‘è¦å–ä¸€æšç¡¬å¸ï¼Œä½†æ˜¯ä¹Ÿä¸èƒ½ä»é™¤ä½ å–çš„è¿™ä¸ªå †ä»¥å¤–çš„å…¶ä»–å †é‡Œå†å–ç¡¬å¸ã€‚</span><br><span class="line">èµ¢å®¶æ˜¯å–å¾—æœ€åä¸€æšç¡¬å¸çš„äººï¼Œæ‰€ä»¥æœ€åæ˜¯æ²¡æœ‰ç¡¬å¸å‰©ä½™çš„ã€‚</span><br></pre></td></tr></table></figure>
<p><img src="resource/game/NimGame.jpg" alt="Nim Game"></p>
<p>ä»è¿™ä¸ªğŸŒ°çœ‹ Aæ˜¯æœ€ç»ˆçš„èƒœè€…ï¼å¦‚æœæˆ‘æ˜¯playerï¼Œå¦‚ä½•æ‰èƒ½å¿…èƒœï¼ï¼ï¼</p>
<p>ä¸¤ç©å®¶åˆ†åˆ«ç§°ä¸ºAå’ŒBï¼ŒAå…ˆèµ°ã€‚</p>
<p>ä¸€å…±ä¸¤å †å„æœ‰ä¸€æšç¡¬å¸ã€‚é‚£ä¹ˆæ˜¾ç„¶ï¼Œç©å®¶Bå¿…èƒœï¼šAå¿…é¡»å–ä¸¤ä¸ªç¡¬å¸é‡Œçš„ä¸€ä¸ªï¼Œç•™ä¸‹ä¸€ä¸ªè¢«Bå–èµ°ã€‚</p>
<p>ç°åœ¨çš„ä¸¤å †åˆ†åˆ«æœ‰2å’Œ1ä¸ªç¡¬å¸ã€‚Aæœ‰å¿…èƒœç­–ç•¥ï¼šä»æœ‰ä¸¤ä¸ªç¡¬å¸çš„é‚£å †å–ä¸€ä¸ªã€‚è¿™æ ·é€ æˆçš„å±€é¢å’Œä¸Šé¢çš„ä¾‹å­ç­‰åŒï¼Œæˆ‘ä»¬çŸ¥é“è¿™æ—¶Aä¼šèµ¢ã€‚</p>
<p>ç¾å›½æ•°å­¦å®¶Charles BoutonåŒæ ·æ„Ÿè§‰åˆ°è¿™ç‚¹å¹¶ä¿ƒä½¿ä»–æ¯«ä¸ç•æƒ§çš„å¼€å§‹åˆ†æè¿™ä¸ªæ¸¸æˆã€‚1902å¹´ä»–æ‰¾åˆ°äº†è¿™ä¸ªçªé—¨â€”â€”åŠå…¶ç²¾å¦™ï¼</p>
<hr>
<p><strong>16ä¸ªè®©ä½ çƒ§è„‘è®©ä½ æ™•çš„æ‚–è®º</strong></p>
<hr>
<p><strong>æœ€å…·äº‰è®®çš„12ä¸ªæ•°å­¦äº‹å®</strong>  </p>
<p><em>ä¸‰é—¨é—®é¢˜ï¼ˆMonty Hall problem</em></p>
<p>å‚èµ›è€…ä¼šçœ‹è§ä¸‰æ‰‡å…³é—­äº†çš„é—¨ï¼Œå…¶ä¸­ä¸€æ‰‡çš„åé¢æœ‰ä¸€è¾†æ±½è½¦ï¼Œé€‰ä¸­åé¢æœ‰è½¦çš„é‚£æ‰‡é—¨å¯èµ¢å¾—è¯¥æ±½è½¦ï¼Œå¦å¤–ä¸¤æ‰‡é—¨åé¢åˆ™å„è—æœ‰ä¸€åªå±±ç¾Šã€‚</p>
<p>å½“å‚èµ›è€…é€‰å®šäº†ä¸€æ‰‡é—¨ï¼Œä½†æœªå»å¼€å¯å®ƒçš„æ—¶å€™ï¼ŒèŠ‚ç›®ä¸»æŒäººå¼€å¯å‰©ä¸‹ä¸¤æ‰‡é—¨çš„å…¶ä¸­ä¸€æ‰‡ï¼Œéœ²å‡ºå…¶ä¸­ä¸€åªå±±ç¾Šã€‚</p>
<p>ä¸»æŒäººå…¶åä¼šé—®å‚èµ›è€…è¦ä¸è¦æ¢å¦ä¸€æ‰‡ä»ç„¶å…³ä¸Šçš„é—¨ã€‚é—®é¢˜æ˜¯ï¼šæ¢å¦ä¸€æ‰‡é—¨ä¼šå¦å¢åŠ å‚èµ›è€…èµ¢å¾—æ±½è½¦çš„æœºä¼šç‡ï¼Ÿ</p>
<p>å¦‚æœä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°çš„æ¡ä»¶ï¼Œå³ä¸»æŒäººæ¸…æ¥šåœ°çŸ¥é“ï¼Œå“ªæ‰‡é—¨åæ˜¯ç¾Šï¼Œé‚£ä¹ˆç­”æ¡ˆæ˜¯ä¼šã€‚</p>
<p>ä¸æ¢é—¨çš„è¯ï¼Œèµ¢å¾—æ±½è½¦çš„å‡ ç‡æ˜¯<code>1/3</code>ã€‚</p>
<p>æ¢é—¨çš„è¯ï¼Œèµ¢å¾—æ±½è½¦çš„å‡ ç‡æ˜¯<code>2/3</code>ã€‚</p>
<hr>
<p><em>0.999â€¦=1</em></p>
<hr>
<p><strong>å½­ç½—æ–¯é˜¶æ¢¯(Penrose stairs)</strong></p>
<p>å½­ç½—æ–¯é˜¶æ¢¯æ˜¯ä¸€ä¸ªæœ‰åçš„å‡ ä½•å­¦æ‚–è®ºï¼ŒæŒ‡çš„æ˜¯ä¸€ä¸ªå§‹ç»ˆå‘ä¸Šæˆ–å‘ä¸‹ä½†å´æ— é™å¾ªç¯çš„é˜¶æ¢¯ï¼Œå¯ä»¥è¢«è§†ä¸ºå½­ç½—æ–¯ä¸‰è§’å½¢çš„ä¸€ä¸ªå˜ä½“ï¼Œåœ¨æ­¤é˜¶æ¢¯ä¸Šæ°¸è¿œæ— æ³•æ‰¾åˆ°æœ€é«˜çš„ä¸€ç‚¹æˆ–è€…æœ€ä½çš„ä¸€ç‚¹ã€‚</p>
<p>å½­ç½—æ–¯é˜¶æ¢¯ä¸å¯èƒ½åœ¨ä¸‰ç»´ç©ºé—´å†…å­˜åœ¨ï¼Œä½†åªè¦æ”¾å…¥æ›´é«˜é˜¶çš„ç©ºé—´ï¼Œå½­ç½—æ–¯é˜¶æ¢¯å°±å¯ä»¥å¾ˆå®¹æ˜“çš„å®ç°ã€‚å¦‚åŒéº¦æ¯”ä¹Œæ–¯åœˆã€å…‹è±å› ç“¶ã€‚</p>
<p><img src="resource/game/PenroseStairs.jpg" alt="Penrose stairs"></p>
<hr>
<p>Reference :</p>
<ol>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzI2NjA3NTc4Ng==&amp;mid=2652077912&amp;idx=1&amp;sn=43d9cf785a9470c278c5d011fce9733b&amp;scene=23&amp;srcid=0429cBdkTUxrcr21zeU7L9i7#rd" target="_blank" rel="external">16ä¸ªè®©ä½ çƒ§è„‘è®©ä½ æ™•çš„æ‚–è®º</a></li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MjM5MTQzNzU2NA==&amp;mid=2651642931&amp;idx=1&amp;sn=b19144eafafb8509f86276ac33a46bca&amp;scene=23&amp;srcid=0724T72b2uKgzYkHuKhI8cuB#rd" target="_blank" rel="external">æœ€å…·äº‰è®®çš„12ä¸ªæ•°å­¦äº‹å®</a></li>
</ol>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-04-13T16:00:00.000Z"><a href="/2016/04/14/live/coffee/">2016-04-14</a></time>
      
      
  
    <h1 class="title"><a href="/2016/04/14/live/coffee/">Coffee</a></h1>
  

    </header>
    <div class="entry">
      
        <h4 id="å’–å•¡èƒå–çš„åŸç†-åŒ–å­¦ååº”-ç‰©ç†ååº”ï¼Ÿï¼"><a href="#å’–å•¡èƒå–çš„åŸç†-åŒ–å­¦ååº”-ç‰©ç†ååº”ï¼Ÿï¼" class="headerlink" title="å’–å•¡èƒå–çš„åŸç†=åŒ–å­¦ååº”+ç‰©ç†ååº”ï¼Ÿï¼"></a><a href="http://www.wtoutiao.com/p/ZfaC0p.html" target="_blank" rel="external">å’–å•¡èƒå–çš„åŸç†=åŒ–å­¦ååº”+ç‰©ç†ååº”ï¼Ÿï¼</a></h4><p>æ•´ä¸ªå’–å•¡å†²æ³¡åˆ†æˆä¸‰ä¸ªé˜¶æ®µï¼šæ¶¦æ¹¿â€”â€”èƒå–â€”â€”æ°´è§£ã€‚</p>
<p>å½“å’–å•¡ç²‰å·²ç»è¢«é«˜æ¸©æ¶¦æ¹¿åï¼Œæ°”ä½“ä»¥åŠæ´»æ³¼çš„æ°”ä½“åŒ–åˆç‰©é¦–å…ˆé€¸æ•£ï¼Œå’–å•¡å†…éƒ¨çš„å¯æº¶æœ‰æœºåˆ†å­åŒæ—¶è¢«æ¸—é€å‹â€œæ‹‰æ‰¯â€åˆ°æ°´ä¸­ï¼Œè¿›è€Œæº¶è§£ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸»è¦æ˜¯å’–å•¡ç²‰ä¸çƒ­æ°´æ¥è§¦å¯¼è‡´çš„ä¸€ç³»åˆ—åŒ–å­¦å’Œç‰©ç†å˜åŒ–ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œèƒå–â€ã€‚<code>ä½†æ˜¯äº‹å®ä¸Šï¼Œè¿™ä¸ªâ€œèƒå–â€ä¸€è¯ä¹Ÿå¯ä»¥ä¸â€œå†²æ³¡â€æ··æ·†ï¼Œå› ä¸ºèƒå–çš„ç¡®æ˜¯å’–å•¡å†²æ³¡ä¸­ç”¨æ—¶æœ€é•¿ã€å½±å“æœ€å¤§çš„ç¯èŠ‚ã€‚</code></p>
<hr>
<h4 id="ä¸–ç•Œå„åœ°çš„äººéƒ½å–å•¥æ ·çš„å’–å•¡"><a href="#ä¸–ç•Œå„åœ°çš„äººéƒ½å–å•¥æ ·çš„å’–å•¡" class="headerlink" title="ä¸–ç•Œå„åœ°çš„äººéƒ½å–å•¥æ ·çš„å’–å•¡?"></a><a href="http://www.wtoutiao.com/p/E2d3UJ.html" target="_blank" rel="external">ä¸–ç•Œå„åœ°çš„äººéƒ½å–å•¥æ ·çš„å’–å•¡?</a></h4><p><code>å¾·å›½ï¼šPharisÃ¤erå’–å•¡</code> åœ¨å’–å•¡é‡ŒåŠ 2ç›å¸æœ—å§†é…’ã€‚é»‘å’–å•¡é…ä¸Šæœ—å§†é…’å’Œç³–ï¼Œå†åŠ ä¸Šç‚¹æ‰“å‘çš„å¥¶æ²¹ï¼Œå‘³é“ç®€ç›´æ²¡å¾—è¯´ï¼</p>
<p><code>è¶Šå—ï¼šé¸¡è›‹å’–å•¡</code> ä¸¤ä¸ªè›‹é»„å’Œç‚¼ä¹³ã€èœ‚èœœå’Œé¦™è‰å…±åŒæ‰“å‘ï¼Œç„¶åå†å€’å…¥åˆšåˆšæ³¡å¥½çš„è¶Šå—å’–å•¡é‡Œã€‚æ‰“å‘çš„è›‹é»„æ¼‚æµ®åœ¨å’–å•¡ä¹‹ä¸Šï¼Œå£æ„Ÿä¸æ»‘ï¼Œå”‡é½¿ç•™é¦™ã€‚</p>
<p><code>ç¾å›½ï¼šGibraltarå’–å•¡</code> æ„å¤§åˆ©æµ“ç¼©å’–å•¡çš„é‡åŠ è‡³2æ¯ï¼Œé…ä¸Šæ‰“å‘çš„ç‰›å¥¶</p>
<p><code>è¥¿ç­ç‰™ï¼šCafe BombÃ³n</code> èœœç³–å’–å•¡  ç”±æµ“ç¼©å’–å•¡å’Œç‚¼ä¹³åˆ¶æˆã€‚</p>
<p><code>åœŸè€³å…¶ï¼šåœŸè€³å…¶å’–å•¡</code> åœŸè€³å…¶å’–å•¡å£¶å†²ç…®ç ”ç£¨æä¸ºç²¾ç»†çš„å’–å•¡ç²‰ï¼Œå£å‘³é¦™ç”œï¼Œå£æ„Ÿé†‡åšã€‚</p>
<p><code>æ³•å›½ï¼šCafe au Lait</code> ç­‰é‡çš„å’–å•¡å’Œæ‰“å‘çš„ç‰›å¥¶åˆ¶æˆã€‚æ³•å›½äººè¿˜å–œæ¬¢åœ¨å’–å•¡é‡ŒåŠ å…¥èŠè‹£ï¼Œä»¥å¢åŠ å’–å•¡çš„ç”œåº¦ã€‚</p>
<p><code>å¢¨è¥¿å“¥ï¼šCafe de Olla</code> ç”¨é”…ç…®çš„å’–å•¡ å°†æ·±åº¦çƒ˜ç„™çš„ã€ç ”ç£¨å¥½çš„å’–å•¡ç²‰ç”¨ç…é”…ç†¬åˆ¶ï¼Œå¹¶åŠ å…¥è‚‰æ¡‚æ£’ã€æ©™çš®å’Œæ£•ç³–ã€‚ç›´åˆ°æ£•ç³–å®Œå…¨èåŒ–ï¼Œå…³ç«æµ¸æ³¡5åˆ†é’Ÿï¼Œå³å¯è¶çƒ­é¥®ç”¨</p>
<p><code>æ„å¤§åˆ©ï¼šRomanoæ„å¤§åˆ©æµ“ç¼©å’–å•¡</code> æµ“ç¼©å’–å•¡é‡ŒåŠ å…¥ä¸€ç‰‡æŸ æª¬</p>
<p><code>æ–°è¥¿å…°å’Œæ¾³å¤§åˆ©äºšï¼šFlat Whiteå’–å•¡</code>  åŒå€æ„å¤§åˆ©æµ“ç¼©å’–å•¡å’Œæ‰“å‘è‡³å£æ„Ÿç»†è…»çš„ç‰›å¥¶åˆ¶æˆã€‚æœ‰äººè¯´è¿™ç§å’–å•¡çš„å£å‘³ä»‹äºå¡å¸ƒå¥‡è¯ºå’Œæ‹¿é“ä¹‹é—´ã€‚</p>
<p><code>å¸Œè…Šï¼šFrappeå’–å•¡</code> å†°é¥®å’–å•¡ï¼Œç”¨é€Ÿæº¶å’–å•¡ã€ç³–ã€åŠ æ°´å¨å£«å¿Œåˆ¶æˆã€‚</p>
<p><code>æ¾³å¤§åˆ©äºšï¼šKaisermelangeå’–å•¡</code> çš‡å¸ç‰¹é¥®ï¼ˆEmperorâ€™s Blendï¼‰ å’–å•¡é‡ŒåŠ å…¥è›‹é»„ã€ç³–å’Œå¹²é‚‘ç™½å…°åœ°</p>
<hr>
<h4 id="å’–å•¡ä¸­çš„é…¸ç©¶ç«Ÿæ˜¯ä»€ä¹ˆä¸œè¥¿ï¼Ÿ"><a href="#å’–å•¡ä¸­çš„é…¸ç©¶ç«Ÿæ˜¯ä»€ä¹ˆä¸œè¥¿ï¼Ÿ" class="headerlink" title="å’–å•¡ä¸­çš„é…¸ç©¶ç«Ÿæ˜¯ä»€ä¹ˆä¸œè¥¿ï¼Ÿ"></a><a href="http://mp.weixin.qq.com/s?__biz=MjM5MTQ3NzE2Mw==&amp;mid=2650452906&amp;idx=2&amp;sn=b8276caa1fb3fcc1a3b1e47b89cd1d65&amp;scene=23&amp;srcid=062550iubemhngkbFlEObyD2#rd" target="_blank" rel="external">å’–å•¡ä¸­çš„é…¸ç©¶ç«Ÿæ˜¯ä»€ä¹ˆä¸œè¥¿ï¼Ÿ</a></h4><p>é…¸æ˜¯ä¸€ç§åŒ–å­¦ç‰©è´¨ï¼Œç‰¹å¾æ˜¯å…·æœ‰é…¸å‘³ã€‚â€œé…¸â€åœ¨è‹±æ–‡ä¸­å«åšâ€œacidâ€ï¼Œæ‹‰ä¸è¯­ä¸­æ„ä¸ºâ€œsourâ€ã€‚åœ¨åŒ–å­¦ä¸Šï¼Œé…¸çš„æ°´æº¶æ¶²PHå€¼å‡å°äº7ï¼Œä¸”æ•°å€¼è¶Šå°é…¸æ€§è¶Šå¤§ã€‚</p>
<p>å¾ˆå¤šé£Ÿç‰©ä¸­å«æœ‰é…¸æ€§åŒ–åˆç‰©ï¼Œå¸¸è§çš„é£Ÿç‰©æœ‰æŸ æª¬ï¼Œé†‹ï¼Œé…¸å¥¶æˆ–å’–å•¡ç­‰ã€‚å•æ˜¯å’–å•¡ï¼Œå°±å«æœ‰æ•°ç™¾ç§é…¸æ€§åŒ–åˆç‰©ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†ä»‹ç»ä¸»è¦å‡ ç§å½±å“äººç±»å£æ„Ÿçš„é…¸ã€‚</p>
<ol>
<li>Citric Acid (æŸ æª¬é…¸) : æŸ æª¬é…¸ä¸»è¦å­˜åœ¨äºæŸ‘æ©˜ç±»æ°´æœä¸­ï¼Œä¹Ÿæ˜¯å¤§å¤šæ•°è”¬èœå’Œæ°´æœä¸­å¸¸è§çš„é…¸ã€‚æŸ æª¬é…¸æ˜¯æœ€å®¹æ˜“è¢«è¯†åˆ«çš„ä¸€ç§é…¸æ€§åŒ–åˆç‰©ã€‚</li>
<li>Malic Acid (è‹¹æœé…¸) : é’è‹¹æœå’Œå¤§é»„ä¸­å«æœ‰å¤§é‡è‹¹æœé…¸ã€‚åœ¨çƒ¹é¥ªç•Œï¼Œè‹¹æœé…¸å¸¸ä¸é’æŸ æª¬è”ç³»åœ¨ä¸€èµ·ï¼Œè‹¹æœé…¸ä¹Ÿè¢«è®¤ä¸ºæ˜¯ä¸€ç§â€œæœªæˆç†Ÿçš„æ°´æœå‘³â€ã€‚å› ä¸ºéšç€æœå®çš„æˆç†Ÿï¼Œå…¶å«æœ‰çš„é…¸åº¦ä¼šæ…¢æ…¢é™ä½ï¼Œæ‰€ä»¥æœªæˆç†Ÿçš„æ°´æœæˆ–ç»¿è‘¡è„ï¼ŒçŒ•çŒ´æ¡ƒå’Œé†‹æ —ç­‰ç»¿é¢œè‰²çš„æ°´æœé…¸åº¦éƒ½æ¯”è¾ƒä½ã€‚</li>
<li>Tartaric Acid (é…’çŸ³é…¸) : è‘¡è„ä¸­é€šå¸¸å«æœ‰è¾ƒå¤šçš„é…’çŸ³é…¸ï¼Œé…’çŸ³æ˜¯é…¿é…’è¿‡ç¨‹ä¸­è‡ªç„¶å‡ºç°çš„ä¸€ç§ç‰©è´¨ï¼Œå…¶ä¸»è¦æˆåˆ†æ˜¯é…’çŸ³é…¸ã€‚é…’çŸ³è¿˜å¯ä»¥ç”¨ä½œé£Ÿå“çš„è†¨æ¾å‰‚ã€‚é…’çŸ³é…¸æœ€çªå‡ºçš„ç‰¹ç‚¹æ˜¯å£æ„Ÿæ˜æ˜¾ã€‚å®ƒä¼šè®©äººé…¸å¾—ç›´æµå£æ°´ï¼Œå¹¶ä¸”å®ƒçš„ä½™éŸµè‹¦æ¶©ã€‚é…’çŸ³é…¸ä¹Ÿæ˜¯è¶…é…¸ç³–æœçš„ä¸»è¦æˆåˆ†ã€‚</li>
<li>Acetic Acid (ä¹™é…¸) : ä¹™é…¸å¾ˆç‰¹åˆ«ï¼Œä¸ä»…å°èµ·æ¥å¾ˆé…¸ï¼Œè€Œä¸”é—»èµ·æ¥ä¹Ÿæœ‰ä¸€è‚¡åˆºé¼»çš„é…¸å‘³ã€‚æµ“åº¦ä½çš„ä¹™é…¸ä¼šæ•£å‘ä»¤äººæ„‰æ‚¦çš„é’æŸ æª¬å‘³ï¼Œæµ“åº¦é«˜çš„ä¹™é…¸å°èµ·æ¥å’Œé—»èµ·æ¥éƒ½æœ‰ä¸€è‚¡å‘é…µçš„å‘³é“ã€‚</li>
</ol>
<p>ä»¥é£å‘³æ˜äº®çš„æµ…çƒ˜ç„™éæ´²å’–å•¡è±†ä¸ºä¾‹ï¼Œé¦–å…ˆèƒå–åæµ‹è¯•åˆ°å®ƒçš„PHå€¼ä¸º4.6ï¼Œå¯ä»¥å¯¹åº”åˆ°è‘¡è„ï¼Œæ¡ƒå­ï¼Œæå­ï¼Œè èç­‰æ°´æœé£å‘³ã€‚æ¥ç€ä½ åœ¨å˜´é‡Œå‘ç°äº†é…’çŸ³é…¸çš„æ¶©å‘³ï¼Œä¹Ÿè®¸ä½ ä¼šå°†è¿™ä¸ªè®¤ä¸ºæ˜¯è‘¡è„é…¸æˆ–å…¶ä»–æœé…¸ã€‚ç„¶åå†æ‰¾åˆ°ç›¸å¯¹åº”çš„æ°´æœé£å‘³ã€‚åŒæ ·ï¼Œå…·æœ‰æŸ æª¬å‘³çš„ä½pHå€¼å¯ç¡®å®šä¸ºâ€œæŸ æª¬é…¸â€ï¼Œå…·æœ‰æŸ æª¬é…¸çš„è¾ƒé«˜pHå€¼å¯èƒ½ä¸ºæ©™å­é£å‘³ã€‚å¦‚æœpHå€¼è¾ƒä½ï¼Œè‹¹æœé…¸å‘³é“è¾ƒå¤šå¯èƒ½æ˜¯é’æŸ æª¬é£å‘³ï¼›å¦‚æœpHå€¼è¾ƒé«˜ï¼Œè‹¹æœé…¸å‘³é“ä¹Ÿè¾ƒå¤šå¯èƒ½æ˜¯å¤§é»„ã€é’è‹¹æœï¼Œæˆ–è‘¡è„æŸšçš„é£å‘³ã€‚</p>
<hr>
<h4 id="100-vs-36-æ•°é‡ä¸è´¨é‡çš„å¯¹å†³"><a href="#100-vs-36-æ•°é‡ä¸è´¨é‡çš„å¯¹å†³" class="headerlink" title="100 vs 36 | æ•°é‡ä¸è´¨é‡çš„å¯¹å†³"></a><a href="http://mp.weixin.qq.com/s?__biz=MjM5MTQ3NzE2Mw==&amp;mid=2650453005&amp;idx=1&amp;sn=09ba8b31f8ee99a9756aade2aa1f781c&amp;scene=23&amp;srcid=0628w1S1CodBU58mZv36O1iD#rd" target="_blank" rel="external">100 vs 36 | æ•°é‡ä¸è´¨é‡çš„å¯¹å†³</a></h4><p>Coffee Aroma Kit T100åˆç§°ä¸ºCoffee Flavor Map(ä»¥ä¸‹ç®€ç§°T100)ï¼Œåœ¨ä»Šå¹´2016SCAAå±•ä¼šä¸­é¦–æ¬¡äº®ç›¸ã€‚SCAAå®˜æ–¹ç½‘ç«™æ¨èè´­ä¹°ã€‚</p>
<p><code>T100</code>æ˜¯ç”±Kicciå…¬å¸æ——ä¸‹çš„SCENTONEæ‰€æ¨å‡ºçš„å’–å•¡é£å‘³é—»é¦™ç“¶ã€‚æ­£å¦‚åå­—ï¼Œæœ‰ç€100ä¸ªé£å‘³ï¼Œå¹¶è¿›è¡Œäº†ç›¸åº”çš„åˆ†ç»„ã€‚åˆ†åˆ«æ˜¯ï¼š</p>
<ol>
<li>Tropical Fruit çƒ­å¸¦æ°´æœç±»</li>
<li>Berry-Like æµ†æœç±»</li>
<li>Citrus &amp; Other Fruits æŸ‘æ©˜ç±»å’Œå…¶ä»–æ°´æœç±»</li>
<li>Stone Fruit æ ¸æœç±»</li>
<li>Cereal &amp; Nut è°·ç‰©ç±»å’Œåšæœç±»</li>
<li>Ceramel &amp; Chocolate ç„¦ç³–ç±»å’Œå·§å…‹åŠ›ç±»</li>
<li>Herb &amp; Flower è‰æœ¬ç±»å’ŒèŠ±é¦™ç±»</li>
<li>Spice é¦™æ–™ç±»</li>
<li>Vegetable è”¬èœç±»</li>
<li>å’¸å‘³ç±»å’Œå…¶ä»–ç±»</li>
</ol>
<p><code>Le Nez Du Cafe 36</code>ï¼ŒSCAAå§”æ‰˜æ³•å›½é¦™æ°´ç ”å‘æœºæ„Ã‰ditions Jean Lenoirè®¾è®¡å’–å•¡é£å‘³ç“¶ã€‚è€æ¿Jeanä¹Ÿè®¾è®¡è¿‡çº¢é…’ä¸“ç”¨çš„é…’é¼»å­Le Nez Du Vin kitï¼Œé…’é¼»å­æœ‰ç€56å‘³ã€‚</p>
<p>å’–å•¡é—»é¦™ç“¶ä¸€ç›´è¢«ç”¨äºCQI Q-Graderçš„åŸ¹è®­å½“ä¸­ï¼Œå¹¶æŒ‰ç…§çƒ˜ç„™è¿‡ç¨‹å½“ä¸­çš„ä¸åŒé˜¶æ®µçš„ååº”è¿›è¡Œäº†åˆ†ç±»ï¼Œåˆ†åˆ«ä¸º4ç»„4ä¸ªå¤§ç±»ï¼Œ12ä¸ªå°ç±»ã€‚</p>
<ol>
<li>Enzymatic é…¶ä¿ƒåŒ–ååº”ç»„-Flowery-Fruity-Herbal</li>
<li>Dry Distillation å¹²é¦åŒ–ååº”ç»„ </li>
<li>Sugar Browning ç„¦ç³–åŒ–ååº”ç»„ Carmelly-Nutty-Chocolaty</li>
<li>Taints ç‘•ç–µé£å‘³/å¼‚å‘³ç»„ Earthy-Fermented-Phenoli</li>
</ol>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">æ²»ç–—å’–å•¡å› ä¸Šç˜¾çš„å”¯ä¸€æ–¹æ³•æ˜¯æé«˜å¯¹å’–å•¡çš„å“é‰´èƒ½åŠ›</span><br><span class="line"></span><br><span class="line">å½“ä¸€ä¸ªäººä¸å†å•çº¯åœ°æ²‰æººäºå’–å•¡å› å¸¦æ¥çš„ç¥ç»å…´å¥‹è€Œèƒ½å¤Ÿæ„ŸçŸ¥é£é…¸ã€æœé¦™ã€é…’å‘³ã€è‹¦æ¶©ã€èŠ³é†‡ç­‰ç»†è‡´çš„é£å‘³æ—¶</span><br><span class="line"></span><br><span class="line">ä»–å°±å†ä¹Ÿä¸ä¼šå¯¹å’–å•¡å› ä¸Šç˜¾äº†â€”â€” ä»–ä¼šå¯¹å’–å•¡æœ¬èº«çš„å‘³é“ä¸Šç˜¾</span><br></pre></td></tr></table></figure>
<hr>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-01-21T08:57:31.000Z"><a href="/2016/01/21/distribution/flink/">2016-01-21</a></time>
      
      
  
    <h1 class="title"><a href="/2016/01/21/distribution/flink/"></a></h1>
  

    </header>
    <div class="entry">
      
        <h2 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h2><p>When the Flink system is started, it bring up the JobManager and one or more TaskManagers. </p>
<p>The JobManager is the coordinator of the Flink system, while the TaskManagers are the workers that execute parts of the parallel programs. When starting the system in local mode, a single JobManager and TaskManager are brought up within the same JVM.</p>
<p><img src="../resource/distribution/flink/process_model.svg" alt="flink process model"></p>
<h3 id="Command-Line-Interface"><a href="#Command-Line-Interface" class="headerlink" title="Command Line Interface"></a>Command Line Interface</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">bin/flink</span><br><span class="line">./flink &lt;ACTION&gt; [OPTIONS] [ARGUMENTS]</span><br><span class="line"></span><br><span class="line">The following actions are available:</span><br><span class="line"></span><br><span class="line">Action &quot;run&quot; compiles and runs a program.</span><br><span class="line"></span><br><span class="line">  Syntax: run [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt;</span><br><span class="line">  &quot;run&quot; action options:</span><br><span class="line">     -c,--class &lt;classname&gt;           Class with the program entry point (&quot;main&quot;</span><br><span class="line">                                      method or &quot;getPlan()&quot; method. Only needed</span><br><span class="line">                                      if the JAR file does not specify the class</span><br><span class="line">                                      in its manifest.</span><br><span class="line">     -m,--jobmanager &lt;host:port&gt;      Address of the JobManager (master) to</span><br><span class="line">                                      which to connect. Specify &apos;yarn-cluster&apos;</span><br><span class="line">                                      as the JobManager to deploy a YARN cluster</span><br><span class="line">                                      for the job. Use this flag to connect to a</span><br><span class="line">                                      different JobManager than the one</span><br><span class="line">                                      specified in the configuration.</span><br><span class="line">     -p,--parallelism &lt;parallelism&gt;   The parallelism with which to run the</span><br><span class="line">                                      program. Optional flag to override the</span><br><span class="line">                                      default value specified in the</span><br><span class="line">                                      configuration.</span><br><span class="line">  Additional arguments if -m yarn-cluster is set:</span><br><span class="line">     -yd,--yarndetached                   Start detached</span><br><span class="line">     -yD &lt;arg&gt;                            Dynamic properties</span><br><span class="line">     -yj,--yarnjar &lt;arg&gt;                  Path to Flink jar file</span><br><span class="line">     -yjm,--yarnjobManagerMemory &lt;arg&gt;    Memory for JobManager Container [in</span><br><span class="line">                                          MB]</span><br><span class="line">     -yn,--yarncontainer &lt;arg&gt;            Number of YARN container to allocate</span><br><span class="line">                                          (=Number of Task Managers)</span><br><span class="line">     -ynm,--yarnname &lt;arg&gt;                Set a custom name for the application</span><br><span class="line">                                          on YARN</span><br><span class="line">     -yq,--yarnquery                      Display available YARN resources</span><br><span class="line">                                          (memory, cores)</span><br><span class="line">     -yqu,--yarnqueue &lt;arg&gt;               Specify YARN queue.</span><br><span class="line">     -ys,--yarnslots &lt;arg&gt;                Number of slots per TaskManager</span><br><span class="line">     -yst,--yarnstreaming                 Start Flink in streaming mode</span><br><span class="line">     -yt,--yarnship &lt;arg&gt;                 Ship files in the specified directory</span><br><span class="line">                                          (t for transfer)</span><br><span class="line">     -ytm,--yarntaskManagerMemory &lt;arg&gt;   Memory per TaskManager Container [in</span><br><span class="line">                                          MB]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Action &quot;info&quot; shows the optimized execution plan of the program (JSON).</span><br><span class="line"></span><br><span class="line">  Syntax: info [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt;</span><br><span class="line">  &quot;info&quot; action options:</span><br><span class="line">     -c,--class &lt;classname&gt;           Class with the program entry point (&quot;main&quot;</span><br><span class="line">                                      method or &quot;getPlan()&quot; method. Only needed</span><br><span class="line">                                      if the JAR file does not specify the class</span><br><span class="line">                                      in its manifest.</span><br><span class="line">     -m,--jobmanager &lt;host:port&gt;      Address of the JobManager (master) to</span><br><span class="line">                                      which to connect. Specify &apos;yarn-cluster&apos;</span><br><span class="line">                                      as the JobManager to deploy a YARN cluster</span><br><span class="line">                                      for the job. Use this flag to connect to a</span><br><span class="line">                                      different JobManager than the one</span><br><span class="line">                                      specified in the configuration.</span><br><span class="line">     -p,--parallelism &lt;parallelism&gt;   The parallelism with which to run the</span><br><span class="line">                                      program. Optional flag to override the</span><br><span class="line">                                      default value specified in the</span><br><span class="line">                                      configuration.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Action &quot;list&quot; lists running and scheduled programs.</span><br><span class="line"></span><br><span class="line">  Syntax: list [OPTIONS]</span><br><span class="line">  &quot;list&quot; action options:</span><br><span class="line">     -m,--jobmanager &lt;host:port&gt;   Address of the JobManager (master) to which</span><br><span class="line">                                   to connect. Specify &apos;yarn-cluster&apos; as the</span><br><span class="line">                                   JobManager to deploy a YARN cluster for the</span><br><span class="line">                                   job. Use this flag to connect to a different</span><br><span class="line">                                   JobManager than the one specified in the</span><br><span class="line">                                   configuration.</span><br><span class="line">     -r,--running                  Show only running programs and their JobIDs</span><br><span class="line">     -s,--scheduled                Show only scheduled programs and their JobIDs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Action &quot;cancel&quot; cancels a running program.</span><br><span class="line"></span><br><span class="line">  Syntax: cancel [OPTIONS] &lt;Job ID&gt;</span><br><span class="line">  &quot;cancel&quot; action options:</span><br><span class="line">     -m,--jobmanager &lt;host:port&gt;   Address of the JobManager (master) to which</span><br><span class="line">                                   to connect. Specify &apos;yarn-cluster&apos; as the</span><br><span class="line">                                   JobManager to deploy a YARN cluster for the</span><br><span class="line">                                   job. Use this flag to connect to a different</span><br><span class="line">                                   JobManager than the one specified in the</span><br><span class="line">                                   configuration.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Please specify an action.</span><br></pre></td></tr></table></figure>
<p>###Configuration</p>
<p>###Web Client<br>Flink provides a web interface to upload jobs, inspect their execution plans, and execute them. </p>
<p>The web interface runs on port 8080 by default.To specify a custom port set the webclient.port property in the conf/flink.yaml.</p>
<p>Jobs are submitted to the JobManager specified by jobmanager.rpc.address and jobmanager.rpc.port.</p>
<p><code>job view</code> Upload a Flink program as a jar file, execute an uploaded program.</p>
<p> jarâ€™s manifest file does not specify the program class, you can specify it before the argument list as:<code>-c &lt;assemblerClass&gt; &lt;programArgs...&gt;</code></p>
<p><code>Plan View</code>  optimized execution plan of the submitted program in the upper half of the page.</p>
<h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>The batch processing APIs of Flink are centered around the DataSet abstraction. </p>
<p>A DataSet is only an abstract representation of a set of data that can contain duplicates.</p>
<table>
<thead>
<tr>
<th>Transformation</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Map</td>
<td></td>
</tr>
<tr>
<td>FlatMap</td>
<td></td>
</tr>
<tr>
<td>MapPartition</td>
<td></td>
</tr>
<tr>
<td>Filter</td>
<td></td>
</tr>
<tr>
<td>Reduce</td>
<td></td>
</tr>
<tr>
<td>ReduceGroup</td>
<td></td>
</tr>
<tr>
<td>Aggregate</td>
<td></td>
</tr>
<tr>
<td>Join</td>
<td></td>
</tr>
<tr>
<td>CoGroup</td>
<td></td>
</tr>
<tr>
<td>Cross</td>
<td></td>
</tr>
<tr>
<td>Union</td>
<td></td>
</tr>
<tr>
<td>Rebalance</td>
<td></td>
</tr>
<tr>
<td>Hash-Partition</td>
<td></td>
</tr>
<tr>
<td>Custom Partitioning</td>
<td></td>
</tr>
<tr>
<td>Sort Partition</td>
<td></td>
</tr>
<tr>
<td>First-n</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="Program-Skeleton"><a href="#Program-Skeleton" class="headerlink" title="Program Skeleton"></a>Program Skeleton</h3><p>Flink programs look like regular Java programs with a main() method. Each program consists of the same basic parts:</p>
<ol>
<li>Obtain an ExecutionEnvironment</li>
<li>Load/create the initial data</li>
<li>Specify transformations on this data</li>
<li>Specify where to put the results</li>
<li>Trigger the program execution</li>
</ol>
<h3 id="DataSet-Transformations"><a href="#DataSet-Transformations" class="headerlink" title="DataSet Transformations"></a>DataSet Transformations</h3><h5 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h5><p>Applies a map function on each element of a DataSet.</p>
<p>One element must be returned by the function.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"></span><br><span class="line">public class SumMapFunction implements MapFunction&lt;Tuple2&lt;Integer, Integer&gt;, Integer&gt; &#123;</span><br><span class="line"></span><br><span class="line">	private static final long serialVersionUID = 3830811031233724943L;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public Integer map(Tuple2&lt;Integer, Integer&gt; tuple) throws Exception &#123;</span><br><span class="line">		return tuple.f0 + tuple.f1;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="FlatMap"><a href="#FlatMap" class="headerlink" title="FlatMap"></a>FlatMap</h5><p>Applies a flat-map function on each element of a DataSet. </p>
<p>This variant of a map function can return arbitrary many result elements (including none) for each input element.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line">import org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line">public class Tokenizer implements FlatMapFunction&lt;String, String&gt; &#123;</span><br><span class="line"></span><br><span class="line">	private static final long serialVersionUID = 2350322299357354023L;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public void flatMap(String line, Collector&lt;String&gt; collector) throws Exception &#123;</span><br><span class="line">		for (String token : line.split(&quot;\\W&quot;)) &#123;</span><br><span class="line">			collector.collect(token);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="MapPartition"><a href="#MapPartition" class="headerlink" title="MapPartition"></a>MapPartition</h5><p>Transforms a parallel partition in a single function call. </p>
<p>The map-partition function gets the partition as Iterable and can produce an arbitrary number of result values. </p>
<p>The number of elements in each partition depends on the degree-of-parallelism and previous operations.</p>
<h5 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h5><h5 id="Project"><a href="#Project" class="headerlink" title="Project"></a>Project</h5><h5 id="Grouped-DataSet"><a href="#Grouped-DataSet" class="headerlink" title="Grouped DataSet"></a>Grouped DataSet</h5><h5 id="Combinable-GroupReduceFunctions"><a href="#Combinable-GroupReduceFunctions" class="headerlink" title="Combinable GroupReduceFunctions"></a>Combinable GroupReduceFunctions</h5><h5 id="GroupCombine-on-a-Grouped-DataSet"><a href="#GroupCombine-on-a-Grouped-DataSet" class="headerlink" title="GroupCombine on a Grouped DataSet"></a>GroupCombine on a Grouped DataSet</h5><h5 id="Aggregate-on-Grouped-Tuple-DataSet"><a href="#Aggregate-on-Grouped-Tuple-DataSet" class="headerlink" title="Aggregate on Grouped Tuple DataSet"></a>Aggregate on Grouped Tuple DataSet</h5><h5 id="Reduce-on-full-DataSet"><a href="#Reduce-on-full-DataSet" class="headerlink" title="Reduce on full DataSet"></a>Reduce on full DataSet</h5><h5 id="GroupReduce-on-full-DataSet"><a href="#GroupReduce-on-full-DataSet" class="headerlink" title="GroupReduce on full DataSet"></a>GroupReduce on full DataSet</h5><h5 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h5><h5 id="Cross"><a href="#Cross" class="headerlink" title="Cross"></a>Cross</h5><h5 id="CoGroup"><a href="#CoGroup" class="headerlink" title="CoGroup"></a>CoGroup</h5><h5 id="Union"><a href="#Union" class="headerlink" title="Union"></a>Union</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1.union(vals2).union(vals3);</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1.union(vals2).union(vals3);</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1.union(vals2).union(vals3);</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1.union(vals2).union(vals3);DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1.union(vals2).union(vals3);DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1.union(vals2).union(vals3);DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1.union(vals2).union(vals3);</span><br></pre></td></tr></table></figure>
<h5 id="First-n"><a href="#First-n" class="headerlink" title="First-n"></a>First-n</h5><h3 id="Flink-Streaming"><a href="#Flink-Streaming" class="headerlink" title="Flink Streaming"></a>Flink Streaming</h3><p><code>Transformations</code></p>
<p>Data transformations transform one or more DataStreams into a new DataStream.</p>
<table>
<thead>
<tr>
<th>Transformation</th>
<th>Structure</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Map</td>
<td>DataStream â†’ DataStream</td>
<td>Takes one element and produces one element.</td>
<td>dataStream.map { x =&gt; x * 2 }</td>
</tr>
<tr>
<td>FlatMap</td>
<td>DataStream â†’ DataStream</td>
<td>Takes one element and produces zero, one, or more elements.</td>
<td>dataStream.flatMap { str =&gt; str.split(â€œ â€œ) }</td>
</tr>
<tr>
<td>Filter</td>
<td>DataStream â†’ DataStream</td>
<td>Evaluates a boolean function for each element and retains those for which the function returns true.</td>
<td>dataStream.filter { _ != 0 }</td>
</tr>
<tr>
<td>KeyBy</td>
<td>DataStream â†’ KeyedStream</td>
<td>Logically partitions a stream into disjoint partitions, each partition containing elements of the same key.</td>
<td>dataStream.keyBy(â€œsomeKeyâ€) dataStream.keyBy(0) </td>
</tr>
<tr>
<td>Reduce</td>
<td>KeyedStream â†’ DataStream</td>
<td></td>
<td>keyedStream.reduce { <em> + </em> }</td>
</tr>
<tr>
<td>Fold</td>
<td>KeyedStream â†’ DataStream</td>
<td></td>
<td>val result: DataStream[String] = keyedStream.fold(â€œstartâ€, (str, i) =&gt; { str + â€œ-â€œ + i })</td>
</tr>
<tr>
<td>Aggregations</td>
<td>KeyedStream â†’ DataStream</td>
<td></td>
<td>keyedStream.sum(0) keyedStream.sum(â€œkeyâ€) keyedStream.min(0) keyedStream.min(â€œkeyâ€) keyedStream.max(0) keyedStream.max(â€œkeyâ€) keyedStream.minBy(0) keyedStream.minBy(â€œkeyâ€) keyedStream.maxBy(0) keyedStream.maxBy(â€œkeyâ€)</td>
</tr>
<tr>
<td>Window</td>
<td>KeyedStream â†’ WindowedStream</td>
<td></td>
<td>dataStream.keyBy(0).window(TumblingTimeWindows.of(Time.of(5, TimeUnit.SECONDS)))</td>
</tr>
<tr>
<td>WindowAll</td>
<td>DataStream â†’ AllWindowedStream</td>
<td></td>
<td>dataStream.windowAll(TumblingTimeWindows.of(Time.of(5, TimeUnit.SECONDS)))</td>
</tr>
<tr>
<td>Window Apply</td>
<td>WindowedStream â†’ DataStream AllWindowedStream â†’ DataStream</td>
<td></td>
<td>windowedStream.apply { applyFunction }</td>
</tr>
<tr>
<td>Window Reduce</td>
<td>WindowedStream â†’ DataStream</td>
<td></td>
<td>windowedStream.reduce { <em> + </em> }</td>
</tr>
<tr>
<td>Window Fold</td>
<td>WindowedStream â†’ DataStream</td>
<td></td>
<td>val result: DataStream[String] = windowedStream.fold(â€œstartâ€, (str, i) =&gt; { str + â€œ-â€œ + i })</td>
</tr>
<tr>
<td>Aggregations on windows</td>
<td>WindowedStream â†’ DataStream</td>
<td></td>
<td>windowedStream.sum(0) windowedStream.sum(â€œkeyâ€) windowedStream.min(0) windowedStream.min(â€œkeyâ€) windowedStream.max(0) windowedStream.max(â€œkeyâ€) windowedStream.minBy(0) windowedStream.minBy(â€œkeyâ€) windowedStream.maxBy(0) windowedStream.maxBy(â€œkeyâ€)</td>
</tr>
<tr>
<td>Union</td>
<td>DataStream* â†’ DataStream</td>
<td></td>
<td>dataStream.union(otherStream1, otherStream2, â€¦)</td>
</tr>
<tr>
<td>Window Join</td>
<td>DataStream,DataStream â†’ DataStream</td>
<td></td>
<td>dataStream.join(otherStream).where(0).equalTo(1).window(TumblingTimeWindows.of(Time.of(3, TimeUnit.SECONDS))).apply { â€¦ }</td>
</tr>
<tr>
<td>Window CoGroup</td>
<td>DataStream,DataStream â†’ DataStream</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Connect</td>
<td>DataStream,DataStream â†’ ConnectedStreams</td>
<td></td>
<td></td>
</tr>
<tr>
<td>CoMap, CoFlatMap</td>
<td>ConnectedStreams â†’ DataStream</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Split</td>
<td>DataStream â†’ SplitStream</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Select</td>
<td>SplitStream â†’ DataStream</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Iterate</td>
<td>DataStream â†’ IterativeStream â†’ DataStream</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Extract Timestamps</td>
<td>DataStream â†’ DataStream</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="Table-API-Relational-Queries"><a href="#Table-API-Relational-Queries" class="headerlink" title="Table API - Relational Queries"></a>Table API - Relational Queries</h4><p>Flink provides an API that allows specifying operations using SQL-like expressions.</p>
<p>Reference :</p>
<ol>
<li><a href="https://flink.apache.org/news/2015/02/09/streaming-example.html" target="_blank" rel="external">Introducing Flink Streaming</a></li>
</ol>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-01-21T08:57:31.000Z"><a href="/2016/01/21/distribution/flume/">2016-01-21</a></time>
      
      
  
    <h1 class="title"><a href="/2016/01/21/distribution/flume/"></a></h1>
  

    </header>
    <div class="entry">
      
        <p>###Flume</p>
<p>#####Source<br><code>Source</code> is to receive data from an external client and store it into the configured Channels. A Source can get an instance of its own ChannelProcessor to process an Event, commited within a Channel local transaction, in serial. In the case of an exception, required Channels will propagate the exception, all Channels will rollback their transaction, but events processed previously on other Channels will remain committed.</p>
<p>Similar to the SinkRunner.PollingRunner Runnable, thereâ€™s a PollingRunner Runnable that executes on a thread created when the Flume framework calls PollableSourceRunner.start(). Each configured PollableSource is associated with its own thread that runs a PollingRunner. This thread manages the PollableSourceâ€™s lifecycle, such as starting and stopping. A PollableSource implementation must implement the start() and stop() methods that are declared in the LifecycleAware interface. The runner of a PollableSource invokes that Sourceâ€˜s process() method. The process() method should check for new data and store it into the Channel as Flume Events.</p>
<p>Note that there are actually two types of Sources. The PollableSource was already mentioned. The other is the EventDrivenSource. The EventDrivenSource, unlike the PollableSource, must have its own callback mechanism that captures the new data and stores it into the Channel. The EventDrivenSources are not each driven by their own thread like the PollableSources are. Below is an example of a custom PollableSource:</p>
<p>#####Channel</p>
<p>#####Sink</p>
<p>#####Exceptions Collection</p>
<ol>
<li><code>Space for commit to queue couldn&#39;t be acquired Sinks are likely not keeping up with sources, or the buffer size is too tight</code></li>
</ol>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-01-21T08:57:31.000Z"><a href="/2016/01/21/distribution/gearpump/">2016-01-21</a></time>
      
      
  
    <h1 class="title"><a href="/2016/01/21/distribution/gearpump/"></a></h1>
  

    </header>
    <div class="entry">
      
        <p>####Gearpump</p>
<p>#####Basic Concepts</p>
<ol>
<li><p>System timestamp and Application timestamp</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   System timestamp is the time of backend cluster system. </span><br><span class="line">   Application timestamp is the time at which message generated.</span><br><span class="line">   ``` </span><br><span class="line"></span><br><span class="line">2. Master, and Worker</span><br></pre></td></tr></table></figure>
<p> Gearpump follow master slave architecture.<br> Every cluster contains one or more Master node, and several worker nodes.<br> Worker node is responsible to manage local resources on single machine.<br> Master node is responsible to manage global resources of the whole cluster.    </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line"></span><br><span class="line">3. Application</span><br></pre></td></tr></table></figure>
<p>Gearpump natively supports Streaming Application types</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">   </span><br><span class="line">4. AppMaster and Executor</span><br></pre></td></tr></table></figure>
</li>
</ol>
   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">   </span><br><span class="line">5. Application Submission Flow</span><br></pre></td></tr></table></figure>
   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">   </span><br><span class="line">6. Streaming Topology, Processor, and Task</span><br></pre></td></tr></table></figure>
<p>   Each application contains a topology, which is a DAG to describe the data flow.<br>   Each node in the DAG is a processor.<br>   An application is a DAG of processors. Each processor handles messages.<br>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">   ![Directed Acyclic Graph](../resource/distribution/gearpump/dag.png)   </span><br><span class="line">   </span><br><span class="line">7. Streaming Task and Partitioner</span><br></pre></td></tr></table></figure></p>
<p>   Task is the minimum unit of parallelism.<br>   ``` </p>
<p>#####Command Line Interface</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-12-31T16:00:00.000Z"><a href="/2016/01/01/language/clojure/">2016-01-01</a></time>
      
      
  
    <h1 class="title"><a href="/2016/01/01/language/clojure/">Clojur</a></h1>
  

    </header>
    <div class="entry">
      
        <h3 id="Clojur"><a href="#Clojur" class="headerlink" title="Clojur"></a>Clojur</h3>
      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-12-31T16:00:00.000Z"><a href="/2016/01/01/distribution/spark/">2016-01-01</a></time>
      
      
  
    <h1 class="title"><a href="/2016/01/01/distribution/spark/">spark</a></h1>
  

    </header>
    <div class="entry">
      
        <h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><p>Apache Spark is a fast and general-purpose cluster computing system. </p>
<p>It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. </p>
<p>It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.</p>
<p>Sparkâ€™s primary abstraction is a distributed collection of items called a Resilient Distributed Dataset (RDD). RDDs can be created from Hadoop InputFormats or by transforming other RDDs. </p>
<p>RDDs have actions, which return values, and transformations, which return pointers to new RDDs.</p>
<p><img src="resource/spark/architecture.jpg" alt="architecture"></p>
<h3 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h3><p>One of the strongest features of Spark is its shell. </p>
<p>The Spark-Shell allows users to type and execute commands in a Unix-Terminal-like fashion. </p>
<ol>
<li><p>To adjust the amount of memory that Spark may use for executing queries you have to set the following environment prior to starting the shell <code>export SPARK_MEM =1 g</code></p>
</li>
<li><p>To  controls the number of worker threads that Spark uses <code>export SPARK_WORKER_INSTANCES =4</code> If you run Spark in local mode you can also set the number of worker threads in one setting as follows: <code>export MASTER = local [32]</code></p>
</li>
</ol>
<hr>
<h3 id="RDD-API"><a href="#RDD-API" class="headerlink" title="RDD API"></a>RDD API</h3><p>RDD is short for Resilient Distributed Dataset. RDDs are the workhorse of the Spark<br>system. As a user, one can consider a RDD as a handle for a collection of individual data partitions which are the result of some computation.</p>
<p>RDDs have <strong>actions</strong>, which return values, and <strong>transformations</strong>, which return pointers to new RDDs.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Î» ~/service/spark-1.6.0/ bin/spark-shell</span><br><span class="line">log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).</span><br><span class="line">log4j:WARN Please initialize the log4j system properly.</span><br><span class="line">log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span><br><span class="line">Using Spark&apos;s repl log4j profile: org/apache/spark/log4j-defaults-repl.properties</span><br><span class="line">To adjust logging level use sc.setLogLevel(&quot;INFO&quot;)</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</span><br><span class="line">Type in expressions to have them evaluated.</span><br><span class="line">Type :help for more information.</span><br><span class="line">16/01/24 21:53:16 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.1.142 instead (on interface en0)</span><br><span class="line">16/01/24 21:53:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address</span><br><span class="line">Spark context available as sc.</span><br><span class="line">SQL context available as sqlContext.</span><br><span class="line"></span><br><span class="line">scala&gt; val list = sc.parallelize(List(1,2,3,4,5,6), 2)</span><br><span class="line">list: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:27</span><br></pre></td></tr></table></figure>
<p><strong>Sparkä¼šæŠŠæ•°æ®éƒ½è½½å…¥åˆ°å†…å­˜ä¹ˆ ? </strong></p>
<p><em>å¦‚æœæ²¡æœ‰ä¸»åŠ¨å¯¹RDDè¿›è¡ŒCache/Persistç­‰ç›¸å…³æ“ä½œï¼ŒRDDä¸è¿‡æ˜¯ä¸€ä¸ªæ¦‚å¿µä¸Šå­˜åœ¨çš„è™šæ‹Ÿæ•°æ®é›†</em></p>
<p>ä¸€ä¸ª RDD æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå‡½æ•°ï¼ŒRDDå˜æ¢ä¸è¿‡æ˜¯å‡½æ•°åµŒå¥—ã€‚ RDDæœ‰ä¸¤ç±»ï¼š</p>
<ol>
<li>è¾“å…¥RDD,å…¸å‹å¦‚KafkaRDDï¼ŒJdbcRDDä»¥åŠHadoopRDDç­‰</li>
<li>è½¬æ¢RDDï¼Œå¦‚MapPartitionsRDD</li>
</ol>
<p><strong>Shuffleçš„æœ¬è´¨æ˜¯ä»€ä¹ˆ ? </strong></p>
<hr>
<h3 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h3><p><code>Spark SQL</code>æ ¸å¿ƒæ˜¯æŠŠå·²æœ‰RDDï¼Œå¸¦ä¸ŠSchemaä¿¡æ¯ï¼Œæ³¨å†Œæˆç±»ä¼¼SQLé‡Œçš„<code>Table</code>ï¼Œå¯¹å…¶è¿›è¡ŒSQLæŸ¥è¯¢ã€‚è¿™é‡Œé¢ä¸»è¦åˆ†ä¸¤éƒ¨åˆ†ï¼Œä¸€æ˜¯ç”ŸæˆSchemaRDï¼ŒäºŒæ˜¯æ‰§è¡ŒæŸ¥è¯¢ã€‚</p>
<p>ä¼ ç»Ÿæ•°æ®åº“å…ˆå°†è¯»å…¥çš„SQLè¯­å¥è¿›è¡Œè§£æï¼Œåˆ†è¾¨å‡ºSQLè¯­å¥ä¸­å“ªäº›è¯æ˜¯å…³é”®å­—ï¼Œå“ªäº›æ˜¯è¡¨è¾¾å¼ï¼Œå“ªäº›æ˜¯Projectionï¼Œå“ªäº›æ˜¯Data Sourceç­‰ç­‰ã€‚</p>
<p>è¿›ä¸€æ­¥åˆ¤æ–­SQLè¯­å¥æ˜¯å¦è§„èŒƒï¼Œä¸è§„èŒƒå°±æŠ¥é”™ï¼Œè§„èŒƒåˆ™æŒ‰ç…§ä¸‹ä¸€æ­¥è¿‡ç¨‹ç»‘å®šã€‚</p>
<p>è¿‡ç¨‹ç»‘å®šæ˜¯å°†SQLè¯­å¥å’Œæ•°æ®åº“çš„æ•°æ®å­—å…¸(åˆ—ï¼Œè¡¨ï¼Œè§†å›¾ç­‰ï¼‰è¿›è¡Œç»‘å®šï¼Œå¦‚æœç›¸å…³çš„Projectionã€Data Sourceç­‰éƒ½å­˜åœ¨ï¼Œå°±è¡¨ç¤ºè¿™ä¸ªSQLè¯­å¥æ˜¯å¯ä»¥æ‰§è¡Œçš„ã€‚</p>
<p>åœ¨æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œæœ‰æ—¶å€™ç”šè‡³ä¸éœ€è¦è¯»å–ç‰©ç†è¡¨å°±å¯ä»¥è¿”å›ç»“æœï¼Œæ¯”å¦‚é‡æ–°è¿è¡Œåˆšè¿è¡Œè¿‡çš„SQLè¯­å¥ï¼Œç›´æ¥ä»æ•°æ®åº“çš„ç¼“å†²æ± ä¸­è·å–è¿”å›ç»“æœã€‚</p>
<p>åœ¨æ•°æ®åº“è§£æçš„è¿‡ç¨‹ä¸­SQLè¯­å¥æ—¶ï¼Œå°†ä¼šæŠŠSQLè¯­å¥è½¬åŒ–æˆä¸€ä¸ªæ ‘å½¢ç»“æ„æ¥è¿›è¡Œå¤„ç†ï¼Œä¼šå½¢æˆä¸€ä¸ªæˆ–å«æœ‰å¤šä¸ªèŠ‚ç‚¹çš„Treeï¼Œç„¶åå†åç»­çš„å¤„ç†æ”¿å¯¹è¯¥Treeè¿›è¡Œä¸€ç³»åˆ—çš„æ“ä½œã€‚</p>
<p>Spark SQLå¯¹SQLè¯­å¥çš„å¤„ç†å’Œå…³ç³»æ•°æ®åº“å¯¹SQLè¯­å¥çš„è§£æé‡‡ç”¨äº†ç±»ä¼¼çš„æ–¹æ³•ï¼Œé¦–å…ˆä¼šå°†SQLè¯­å¥è¿›è¡Œè§£æï¼Œç„¶åå½¢æˆä¸€ä¸ªTreeï¼Œåç»­å¦‚ç»‘å®šã€ä¼˜åŒ–ç­‰å¤„ç†è¿‡ç¨‹éƒ½æ˜¯å¯¹Treeçš„æ“ä½œï¼Œè€Œæ“ä½œæ–¹æ³•æ˜¯é‡‡ç”¨Ruleï¼Œé€šè¿‡æ¨¡å¼åŒ¹é…ï¼Œå¯¹ä¸åŒç±»å‹çš„èŠ‚ç‚¹é‡‡ç”¨ä¸åŒçš„æ“ä½œã€‚SparkSQLæœ‰ä¸¤ä¸ªåˆ†æ”¯ï¼ŒSQLContextå’ŒHiveContextã€‚</p>
<p>SQLContextç°åœ¨åªæ”¯æŒSQLè¯­æ³•è§£æå™¨ï¼ˆCatalyst)ï¼ŒHiveContextæ”¯æŒSQLè¯­æ³•å’ŒHiveContextè¯­æ³•è§£æå™¨ã€‚</p>
<p>Spark SQL is a Spark module for structured data processing. </p>
<p>It provides a programming abstraction called DataFrames and can also act as distributed SQL query engine.</p>
<ol>
<li>DataFrame: a distributed collection of data organized into named columns.</li>
</ol>
<p><code>Spark SQL</code> å¯¹ SQL è¯­å¥çš„å¤„ç†å’Œå…³ç³»å‹æ•°æ®åº“å¯¹ SQL è¯­å¥çš„å¤„ç†é‡‡ç”¨äº†ç±»ä¼¼çš„æ–¹æ³•ã€‚é¦–å…ˆä¼šå°† SQL è¯­å¥è¿›è¡Œè§£æ(Parse),ç„¶åå½¢æˆä¸€ä¸ª Tree,åœ¨åç»­çš„å¦‚ç»‘å®šã€ä¼˜åŒ–ç­‰å¤„ ç†è¿‡ç¨‹éƒ½æ˜¯å¯¹ Tree çš„æ“ä½œ,è€Œæ“ä½œçš„æ–¹æ³•æ˜¯é‡‡ç”¨ Rule,é€šè¿‡æ¨¡å¼åŒ¹é…,å¯¹ä¸åŒç±»å‹çš„èŠ‚ç‚¹ é‡‡ç”¨ä¸åŒçš„æ“ä½œã€‚</p>
<p><img src="resource/spark/SparkSQLOverview.png" alt="SparkSQL-Overview"></p>
<p><code>Tree</code></p>
<p><code>TreeNode</code> å¯ä»¥ç»†åˆ†æˆä¸‰ç§ç±»å‹çš„ Node : </p>
<ol>
<li>UnaryNode ä¸€å…ƒèŠ‚ç‚¹,å³åªæœ‰ä¸€ä¸ªå­èŠ‚ç‚¹ã€‚</li>
<li>BinaryNode äºŒå…ƒèŠ‚ç‚¹,å³æœ‰å·¦å³å­èŠ‚ç‚¹çš„äºŒå‰èŠ‚ç‚¹ã€‚</li>
<li>LeafNode å¶å­èŠ‚ç‚¹,æ²¡æœ‰å­èŠ‚ç‚¹çš„èŠ‚ç‚¹ã€‚</li>
</ol>
<p><code>Rule</code> </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">         SQL Parser                          Analyer                        Optimizer</span><br><span class="line">SQL Text ----------&gt; Unresolved Logical Plan -------&gt; Resolved Logical Plan ---------&gt;</span><br><span class="line"></span><br><span class="line">                       Spark Plan                 prepare for execution                             execute</span><br><span class="line">Optimized Logical Plan ----------&gt; Physical Plan ----------------------&gt;  Executable Physical Plan --------&gt;</span><br><span class="line"></span><br><span class="line">         toRDD</span><br><span class="line">Execute -------&gt; SchemaRDD</span><br></pre></td></tr></table></figure>
<p><code>Catalyst</code> </p>
<ol>
<li>SQL Parse å®Œæˆ SQL è¯­å¥è¯­æ³•è§£æåŠŸèƒ½</li>
<li>Analyzer ä¸»è¦å®Œæˆç»‘å®šå·¥ä½œ,å°†ä¸åŒæ¥æºçš„ Unresolved LogicalPlan å’Œæ•°æ®å…ƒæ•°æ®è¿›è¡Œç»‘å®š,ç”Ÿæˆ resolved LogicalPlanã€‚</li>
<li>Optimizer å¯¹ Resolved LogicalPlan è¿›è¡Œä¼˜åŒ–,ç”Ÿæˆ Optimized LogicalPlanã€‚</li>
<li>Planner å°† LogicalPlan è½¬æ¢æˆ PhysicalPlanã€‚</li>
<li>CostModel ä¸»è¦æ ¹æ®è¿‡å»çš„æ€§èƒ½ç»Ÿè®¡æ•°æ®,é€‰æ‹©æœ€ä½³çš„ç‰©ç†æ‰§è¡Œè®¡åˆ’ã€‚</li>
</ol>
<hr>
<h3 id="Streaming"><a href="#Streaming" class="headerlink" title="Streaming"></a>Streaming</h3><p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Sparkâ€™s machine learning and graph processing algorithms on data streams.</p>
<p>Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data.</p>
<p>Internally, a DStream is represented as a sequence of RDDs.</p>
<p><img src="resource/spark/streaming-dstream-window.png" alt="streaming-dstream-window"></p>
<p><img src="resource/spark/words-dstream.png" alt="streaming-dstream-window"></p>
<p><img src="resource/spark/lines-dstream.png" alt="streaming-dstream-window"></p>
<hr>
<h3 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h3><p>GraphXæ˜¯ä¸€ä¸ªæ–°çš„ Spark APIï¼Œå®ƒç”¨äºå›¾å’Œå¹¶è¡Œå›¾è®¡ç®—ã€‚GraphXé€šè¿‡å¼•å…¥<code>Resilient Distributed Property Graph</code> (å¸¦æœ‰é¡¶ç‚¹å’Œè¾¹å±æ€§æœ‰å‘å¤šé‡å›¾)ï¼Œæ¥æ‰©å±•Spark RDDã€‚ä¸ºäº†æ”¯æŒå›¾è®¡ç®—ï¼ŒGraphXå…¬å¼€ä¸€ç»„åŸºæœ¬çš„åŠŸèƒ½æ“ä½œä»¥åŠPregel APIçš„ä¼˜åŒ–ã€‚</p>
<p>ä»ç¤¾äº¤ç½‘ç»œåˆ°è¯­è¨€å»ºæ¨¡ï¼Œä¸æ–­å¢é•¿çš„è§„æ¨¡å’Œå›¾å½¢æ•°æ®çš„é‡è¦æ€§å·²ç»æ¨åŠ¨äº†è®¸å¤šæ–°çš„graph-parallelç³»ç»Ÿï¼ˆå¦‚Giraphå’ŒGraphLabï¼‰çš„å‘å±•ã€‚é€šè¿‡é™åˆ¶å¯è¡¨è¾¾çš„è®¡ç®—ç±»å‹å’Œå¼•å…¥æ–°çš„æŠ€æœ¯æ¥åˆ’åˆ†å’Œåˆ†é…å›¾ï¼Œè¿™äº›ç³»ç»Ÿå¯ä»¥é«˜æ•ˆåœ°æ‰§è¡Œå¤æ‚çš„å›¾å½¢ç®—æ³•ï¼Œæ¯”ä¸€èˆ¬çš„data-parallelç³»ç»Ÿå¿«å¾ˆå¤šã€‚</p>
<p><img src="resource/spark/GraphX.png" alt="GraphX"></p>
<p><code>å±æ€§å›¾</code>æ˜¯ä¸€ä¸ªæœ‰å‘å¤šé‡å›¾ï¼Œå®ƒå¸¦æœ‰è¿æ¥åˆ°æ¯ä¸ªé¡¶ç‚¹å’Œè¾¹çš„ç”¨æˆ·å®šä¹‰çš„å¯¹è±¡ã€‚æœ‰å‘å¤šé‡å›¾ä¸­å¤šä¸ªå¹¶è¡Œ(parallel)çš„è¾¹å…±äº«ç›¸åŒçš„æºå’Œç›®çš„åœ°é¡¶ç‚¹ã€‚</p>
<p>æ”¯æŒå¹¶è¡Œè¾¹çš„èƒ½åŠ›ç®€åŒ–äº†å»ºæ¨¡åœºæ™¯ï¼Œè¿™ä¸ªåœºæ™¯ä¸­ï¼Œç›¸åŒçš„é¡¶ç‚¹å­˜åœ¨å¤šç§å…³ç³»ã€‚æ¯ä¸ªé¡¶ç‚¹ç”±ä¸€ä¸ªå”¯ä¸€çš„64ä½é•¿çš„æ ‡è¯†ç¬¦ï¼ˆVertexIDï¼‰ä½œä¸ºkeyã€‚GraphXå¹¶æ²¡æœ‰å¯¹é¡¶ç‚¹æ ‡è¯†å¼ºåŠ ä»»ä½•æ’åºã€‚</p>
<p>åŒæ ·ï¼Œé¡¶ç‚¹æ‹¥æœ‰ç›¸åº”çš„æºå’Œç›®çš„é¡¶ç‚¹æ ‡è¯†ç¬¦ã€‚</p>
<p>å±æ€§å›¾é€šè¿‡vertex(VD)å’Œedge(ED)ç±»å‹å‚æ•°åŒ–ï¼Œè¿™äº›ç±»å‹æ˜¯åˆ†åˆ«ä¸æ¯ä¸ªé¡¶ç‚¹å’Œè¾¹ç›¸å…³è”çš„å¯¹è±¡çš„ç±»å‹ã€‚</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">val vertexRDD: RDD[(Long, (String, Int))] = sc.parallelize(vertexArray)</span><br><span class="line">val edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArray)</span><br><span class="line"></span><br><span class="line">val vertexArray = Array(</span><br><span class="line">  (1L, (&quot;Alice&quot;, 28)),</span><br><span class="line">  (2L, (&quot;Bob&quot;, 27)),</span><br><span class="line">  (3L, (&quot;Charlie&quot;, 65)),</span><br><span class="line">  (4L, (&quot;David&quot;, 42)),</span><br><span class="line">  (5L, (&quot;Ed&quot;, 55)),</span><br><span class="line">  (6L, (&quot;Fran&quot;, 50)))</span><br><span class="line">  </span><br><span class="line">val edgeArray = Array(</span><br><span class="line">  Edge(2L, 1L, 7),</span><br><span class="line">  Edge(2L, 4L, 2),</span><br><span class="line">  Edge(3L, 2L, 4),</span><br><span class="line">  Edge(3L, 6L, 3),</span><br><span class="line">  Edge(4L, 1L, 1),</span><br><span class="line">  Edge(5L, 2L, 2),</span><br><span class="line">  Edge(5L, 3L, 8),</span><br><span class="line">  Edge(5L, 6L, 3))</span><br><span class="line"></span><br><span class="line">val vertexRDD: RDD[(Long, (String, Int))] = sc.parallelize(vertexArray)</span><br><span class="line">val edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArray)</span><br><span class="line"></span><br><span class="line">val graph: Graph[(String, Int), Int] = Graph(vertexRDD, edgeRDD)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="MLlib"><a href="#MLlib" class="headerlink" title="MLlib"></a>MLlib</h3><p>MLlib æ˜¯ Spark å¯¹å¸¸ç”¨çš„æœºå™¨å­¦ä¹ ç®—æ³•çš„å®ç°åº“,åŒæ—¶åŒ…æ‹¬ç›¸å…³çš„æµ‹è¯•å’Œæ•°æ®ç”Ÿæˆå™¨ã€‚</p>
<p>Spark çš„è®¾è®¡åˆè¡·å°±æ˜¯ä¸ºäº†æ”¯æŒä¸€äº›è¿­ä»£çš„ Job, è¿™æ­£å¥½æ˜¯å¾ˆå¤šæœºå™¨å­¦ä¹ ç®—æ³•çš„æ€§è´¨ã€‚</p>
<p>MLlib ç›®å‰æ”¯æŒå››ç§å¸¸è§çš„æœºå™¨å­¦ä¹ é—®é¢˜: åˆ†ç±», å›å½’, èšç±»å’ŒååŒè¿‡æ»¤ã€‚</p>
<hr>
<h3 id="R"><a href="#R" class="headerlink" title="R"></a>R</h3><p>SparkR provides a distributed data frame implementation that supports operations like selection, filtering, aggregation etc.</p>
<p>SparkR also supports distributed machine learning using MLlib.</p>
<hr>
<h3 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h3><p>There are two deploy modes that can be used to launch Spark applications on YARN.</p>
<p><strong>In cluster mode</strong>, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. </p>
<p><strong>In client mode</strong>, the driver runs in the client process, and the application master is only used for requesting resources from YARN.</p>
<p>In YARN mode the ResourceManagerâ€™s address is picked up from the Hadoop configuration. Thus, the <code>--master</code> parameter is yarn.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] &lt;app jar&gt; [app options]</span><br></pre></td></tr></table></figure>
<p><code>å®¢æˆ·ç«¯è¿›è¡Œæ“ä½œ</code></p>
<ol>
<li>æ ¹æ®yarnConfæ¥åˆå§‹åŒ–yarnClientï¼Œå¹¶å¯åŠ¨yarnClient</li>
<li>åˆ›å»ºå®¢æˆ·ç«¯Applicationï¼Œå¹¶è·å–Applicationçš„IDï¼Œè¿›ä¸€æ­¥åˆ¤æ–­é›†ç¾¤ä¸­çš„èµ„æºæ˜¯å¦æ»¡è¶³executorå’ŒApplicationMasterç”³è¯·çš„èµ„æºï¼Œå¦‚æœä¸æ»¡è¶³åˆ™æŠ›å‡ºIllegalArgumentException</li>
<li>è®¾ç½®èµ„æºã€ç¯å¢ƒå˜é‡ï¼šå…¶ä¸­åŒ…æ‹¬äº†è®¾ç½®Applicationçš„Stagingç›®å½•ã€å‡†å¤‡æœ¬åœ°èµ„æºã€è®¾ç½®Applicationå…¶ä¸­çš„ç¯å¢ƒå˜é‡ã€åˆ›å»ºContainerå¯åŠ¨çš„Contextç­‰</li>
<li>è®¾ç½®Applicationæäº¤çš„Contextï¼ŒåŒ…æ‹¬è®¾ç½®åº”ç”¨çš„åå­—ã€é˜Ÿåˆ—ã€AMçš„ç”³è¯·çš„Containerã€æ ‡è®°è¯¥ä½œä¸šçš„ç±»å‹ä¸ºSpark</li>
<li>ç”³è¯·Memoryï¼Œå¹¶æœ€ç»ˆé€šè¿‡yarnClient.submitApplicationå‘ResourceManageræäº¤è¯¥Application</li>
</ol>
<p><code>æäº¤åˆ°YARNé›†ç¾¤</code></p>
<ol>
<li>è¿è¡ŒApplicationMasterçš„runæ–¹æ³•</li>
<li>è®¾ç½®å¥½ç›¸å…³çš„ç¯å¢ƒå˜é‡</li>
<li>åˆ›å»ºamClientï¼Œå¹¶å¯åŠ¨</li>
<li>åœ¨Spark UIå¯åŠ¨ä¹‹å‰è®¾ç½®Spark UIçš„AmIpFilter</li>
<li>åœ¨startUserClasså‡½æ•°ä¸“é—¨å¯åŠ¨äº†ä¸€ä¸ªçº¿ç¨‹æ¥å¯åŠ¨ç”¨æˆ·æäº¤çš„Applicationï¼Œä¹Ÿå°±æ˜¯å¯åŠ¨äº†Driverã€‚åœ¨Driverä¸­å°†ä¼šåˆå§‹åŒ–SparkContext</li>
<li>ç­‰å¾…SparkContextåˆå§‹åŒ–å®Œæˆï¼Œæœ€å¤šç­‰å¾…spark.yarn.applicationMaster.waitTriesæ¬¡æ•°ï¼Œå¦‚æœç­‰å¾…äº†çš„æ¬¡æ•°è¶…è¿‡äº†é…ç½®çš„ï¼Œç¨‹åºå°†ä¼šé€€å‡ºï¼›å¦åˆ™ç”¨SparkContextåˆå§‹åŒ–yarnAllocator</li>
<li>å½“SparkContextã€Driveråˆå§‹åŒ–å®Œæˆçš„æ—¶å€™ï¼Œé€šè¿‡amClientå‘ResourceManageræ³¨å†ŒApplicationMaster</li>
<li>åˆ†é…å¹¶å¯åŠ¨Executeorsã€‚åœ¨å¯åŠ¨Executeorsä¹‹å‰ï¼Œå…ˆè¦é€šè¿‡yarnAllocatorè·å–åˆ°numExecutorsä¸ªContainerï¼Œç„¶ååœ¨Containerä¸­å¯åŠ¨Executeors</li>
<li>æœ€åï¼ŒTaskå°†åœ¨oarseGrainedExecutorBackendé‡Œé¢è¿è¡Œï¼Œç„¶åè¿è¡ŒçŠ¶å†µä¼šé€šè¿‡Akkaé€šçŸ¥CoarseGrainedSchedulerï¼Œç›´åˆ°ä½œä¸šè¿è¡Œå®Œæˆ</li>
</ol>
<hr>
<p><strong>Config</strong></p>
<p><strong>Application Properties</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.app.name</td>
<td>The name of your application.</td>
</tr>
<tr>
<td>spark.driver.cores</td>
<td>Number of cores to use for the driver process (only cluster mode).</td>
</tr>
<tr>
<td>spark.driver.maxResultSize</td>
<td>Limit of total size of serialized results of all partitions for each Spark action.</td>
</tr>
<tr>
<td>spark.driver.memory</td>
<td>Memory to use for the driver process</td>
</tr>
<tr>
<td>spark.executor.memory</td>
<td>Memory to use per executor process</td>
</tr>
<tr>
<td>spark.extraListeners</td>
<td></td>
</tr>
<tr>
<td>spark.local.dir</td>
<td></td>
</tr>
<tr>
<td>spark.logConf</td>
<td></td>
</tr>
<tr>
<td>spark.master</td>
<td>The cluster manager to connect </td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.yarn.am.memory</td>
<td>Amount of memory to use for the YARN Application Master in client mode</td>
</tr>
<tr>
<td>spark.driver.cores</td>
<td>Number of cores used by the driver in YARN cluster mode.</td>
</tr>
<tr>
<td>spark.yarn.am.cores</td>
<td>Number of cores to use for the YARN Application Master in client mode.</td>
</tr>
<tr>
<td>spark.yarn.am.waitTime</td>
<td>In cluster mode, time for the YARN Application Master to wait for the SparkContext to be initialized. In client mode, time for the YARN Application Master to wait for the driver to connect to it.</td>
</tr>
<tr>
<td>spark.yarn.submit.file.replication</td>
<td>HDFS replication level for the files uploaded into HDFS. </td>
</tr>
<tr>
<td>spark.yarn.preserve.staging.files</td>
<td>Set to true to preserve the staged files at the end of the job.</td>
</tr>
<tr>
<td>spark.yarn.scheduler.heartbeat.interval-ms</td>
<td>Spark application master heartbeats into the YARN ResourceManager interval.</td>
</tr>
<tr>
<td>spark.yarn.scheduler.initial-allocation.interval</td>
<td>The initial interval in which the Spark application master eagerly heartbeats to the YARN ResourceManager when there are pending container allocation requests. </td>
</tr>
<tr>
<td>spark.yarn.max.executor.failures</td>
<td>The maximum number of executor failures before failing the application.</td>
</tr>
<tr>
<td>spark.yarn.historyServer.address</td>
<td>The address of the Spark history server</td>
</tr>
<tr>
<td>spark.yarn.dist.archives</td>
<td>Comma separated list of archives to be extracted into the working directory of each executor.</td>
</tr>
<tr>
<td>spark.yarn.dist.files</td>
<td>Comma-separated list of files to be placed in the working directory of each executor.</td>
</tr>
<tr>
<td>spark.executor.instances</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.executor.memoryOverhead</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.driver.memoryOverhead</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.memoryOverhead</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.port</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.queue</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.jar</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.access.namenodes</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.appMasterEnv.[EnvironmentVariableName]</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.containerLauncherMaxThreads</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.extraJavaOptions</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.extraLibraryPath</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.maxAppAttempts</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.attemptFailuresValidityInterval</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.submit.waitAppCompletion</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.nodeLabelExpression</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.executor.nodeLabelExpression</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.tags</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.keytab</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.principal</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.config.gatewayPath</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.config.replacementPath</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.security.tokens.${service}.enabled</td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Shuffle Behavior</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.reducer.maxSizeInFlight</td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Spark UI</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Compression and Serialization</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Memory Management</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Execution Behavior</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><img src="resource/spark//spark_on_yarn.jpg" alt="spark_on_yarn"></p>
<hr>
<h3 id="PySpark"><a href="#PySpark" class="headerlink" title="PySpark"></a>PySpark</h3><h3 id="Config"><a href="#Config" class="headerlink" title="Config"></a>Config</h3><table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.app.name</td>
<td>The name of your application. </td>
</tr>
<tr>
<td>spark.driver.cores</td>
<td>Number of cores to use for the driver process, only in cluster mode.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>Spark applications run as independent sets of processes on a cluster,coordinated by the SparkContext (<code>driver program</code>).</p>
<p>SparkContext can connect to several types of cluster managers.</p>
<p><img src="resource/spark/cluster-overview.png" alt="architecture"></p>
<table>
<thead>
<tr>
<th>Term</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Application</td>
<td>User program built on Spark.</td>
</tr>
<tr>
<td>Driver program</td>
<td>The process running the main() function of the application and creating the SparkContext</td>
</tr>
<tr>
<td>Cluster manager</td>
<td>An external service for acquiring resources on the cluster</td>
</tr>
<tr>
<td>Deploy mode</td>
<td>Distinguishes where the driver process runs.</td>
</tr>
<tr>
<td>Worker node</td>
<td>Any node that can run application code in the cluster</td>
</tr>
<tr>
<td>Executor</td>
<td>A process that runs tasks and keeps data in memory or disk storage.</td>
</tr>
<tr>
<td>Task</td>
<td>A unit of work that will be sent to one executor</td>
</tr>
<tr>
<td>Job</td>
<td>A parallel computation consisting of multiple tasks</td>
</tr>
<tr>
<td>Stage</td>
<td>Each job gets divided into smaller sets of tasks</td>
</tr>
</tbody>
</table>
<p><code>Spark Driver æ˜¯æ‰§è¡Œmain()æ–¹æ³•çš„è¿›ç¨‹</code></p>
<p><code>æ‰§è¡Œå™¨èŠ‚ç‚¹</code></p>
<h3 id="DataFrame-amp-DataSet"><a href="#DataFrame-amp-DataSet" class="headerlink" title="DataFrame &amp; DataSet"></a>DataFrame &amp; DataSet</h3><p>Reference:</p>
<ol>
<li><a href="http://www.infoq.com/cn/articles/2015-Review-Spark" target="_blank" rel="external">è§£è¯»2015ä¹‹Sparkç¯‡ï¼šæ–°ç”Ÿæ€ç³»ç»Ÿçš„å½¢æˆ</a></li>
<li><a href="http://www.infoq.com/cn/presentations/gc-tuning-of-spark-application?utm_campaign=rightbar_v2&amp;utm_source=infoq&amp;utm_medium=presentations_link&amp;utm_content=link_text" target="_blank" rel="external">Sparkåº”ç”¨çš„GCè°ƒä¼˜</a></li>
<li><a href="http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html" target="_blank" rel="external">The RDD API By Example</a></li>
<li><a href="https://databricks.com/blog" target="_blank" rel="external">Databricks Blog</a></li>
<li><a href="https://databricks.gitbooks.io/databricks-spark-reference-applications/content/index.html" target="_blank" rel="external">databricks.gitbooks.io</a></li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzA5MTc0NTMwNQ==&amp;mid=402777570&amp;idx=1&amp;sn=b06881f5d374cc181784cd8ba31893ad&amp;scene=23&amp;srcid=0318eIjnKdNTXNnQxbXTVpyp#rd" target="_blank" rel="external">ä¸€ä¸ªSparkSQLä½œä¸šçš„ä¸€ç”Ÿ</a></li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzA5MTc0NTMwNQ==&amp;mid=402803628&amp;idx=1&amp;sn=bd72f7e43ddefb0946121778ac161ab5&amp;scene=23&amp;srcid=0318z3Rle2LCaZomw5S45p9D#rd" target="_blank" rel="external">Sparkç”Ÿæ€é¡¶çº§é¡¹ç›®æ±‡æ€»</a></li>
<li><a href="http://ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html" target="_blank" rel="external">graph analytics with graphx</a></li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzA5MTc0NTMwNQ==&amp;mid=2650713520&amp;idx=1&amp;sn=9e27441611ac6411eca5c13b489a6d37&amp;scene=23&amp;srcid=0421d2KxIwEnxywTxa2W06TT#rd" target="_blank" rel="external">Sparkä¼šæŠŠæ•°æ®éƒ½è½½å…¥åˆ°å†…å­˜ä¹ˆ</a></li>
<li><a href="http://www.kancloud.cn/kancloud/spark-programming-guide/51557" target="_blank" rel="external">GraphXç¼–ç¨‹æŒ‡å—</a></li>
<li><a href="https://snap.stanford.edu/data/" target="_blank" rel="external">Stanford Large Network Dataset Collection</a></li>
<li><a href="https://github.com/lw-lin/CoolplaySpark" target="_blank" rel="external">é…·ç© Spark</a></li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzA3OTAxMDQzNQ==&amp;mid=2650607874&amp;idx=1&amp;sn=3626d0d3d16be6236cfc9922eb0cd295&amp;scene=23&amp;srcid=0524320Jr36VjpcvuRJsenht#rd" target="_blank" rel="external">SparkçŸ¥è¯†ä½“ç³»å®Œæ•´è§£è¯»</a></li>
<li><a href="https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html" target="_blank" rel="external">Deep Dive into Spark SQLâ€™s Catalyst Optimizer</a></li>
<li><a href="http://www.csdn.net/article/1970-01-01/2824823" target="_blank" rel="external">GCè°ƒä¼˜åœ¨Sparkåº”ç”¨ä¸­çš„å®è·µ</a></li>
<li><a href="https://cs.stanford.edu/~matei/" target="_blank" rel="external">Matei Zaharia</a></li>
<li><a href="http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf" target="_blank" rel="external">PDF Spark SQL: Relational Data Processing in Spark</a></li>
</ol>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





<nav id="pagination">
  
    <a href="/page/7/" class="alignleft prev">Prev</a>
  
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:yoursite.com">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">Categories</h3>
  <ul class="entry">
  
    <li><a href="/categories/Algorithms/">Algorithms</a><small>1</small></li>
  
    <li><a href="/categories/Design/">Design</a><small>1</small></li>
  
    <li><a href="/categories/Distributed/">Distributed</a><small>9</small></li>
  
    <li><a href="/categories/Language/">Language</a><small>4</small></li>
  
    <li><a href="/categories/Machine-Learning/">Machine Learning</a><small>1</small></li>
  
    <li><a href="/categories/OP/">OP</a><small>1</small></li>
  
    <li><a href="/categories/Storage/">Storage</a><small>2</small></li>
  
    <li><a href="/categories/live/">live</a><small>3</small></li>
  
    <li><a href="/categories/machine-learning/">machine_learning</a><small>1</small></li>
  
  </ul>
</div>


  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/tags/Algorithms/">Algorithms</a><small>1</small></li>
  
    <li><a href="/tags/Alluxio/">Alluxio</a><small>1</small></li>
  
    <li><a href="/tags/Clojure/">Clojure</a><small>1</small></li>
  
    <li><a href="/tags/Design/">Design</a><small>1</small></li>
  
    <li><a href="/tags/Hadoop/">Hadoop</a><small>5</small></li>
  
    <li><a href="/tags/JVM/">JVM</a><small>1</small></li>
  
    <li><a href="/tags/Java/">Java</a><small>1</small></li>
  
    <li><a href="/tags/Linux/">Linux</a><small>1</small></li>
  
    <li><a href="/tags/Machine-Learning/">Machine Learning</a><small>1</small></li>
  
    <li><a href="/tags/MySQL/">MySQL</a><small>1</small></li>
  
    <li><a href="/tags/RocksDB/">RocksDB</a><small>1</small></li>
  
    <li><a href="/tags/Scala/">Scala</a><small>1</small></li>
  
    <li><a href="/tags/Streaming-Process/">Streaming Process</a><small>1</small></li>
  
    <li><a href="/tags/TensorFlow/">TensorFlow</a><small>1</small></li>
  
    <li><a href="/tags/ZooKeeper/">ZooKeeper</a><small>1</small></li>
  
    <li><a href="/tags/coffee/">coffee</a><small>1</small></li>
  
    <li><a href="/tags/fitness/">fitness</a><small>1</small></li>
  
    <li><a href="/tags/game/">game</a><small>1</small></li>
  
    <li><a href="/tags/spark/">spark</a><small>1</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2016 Darion Yaphet
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>
