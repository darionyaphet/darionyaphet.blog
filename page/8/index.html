<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Page 8 | darion.johannes.yaphet</title>
  <meta name="author" content="Darion Yaphet">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="darion.johannes.yaphet"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="darion.johannes.yaphet" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">darion.johannes.yaphet</a></h1>
  <h2><a href="/">long is the way and hard  that out of Hell leads up to light</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper">
  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-04-18T16:00:00.000Z"><a href="/2016/04/19/live/game/">2016-04-19</a></time>
      
      
  
    <h1 class="title"><a href="/2016/04/19/live/game/">Game</a></h1>
  

    </header>
    <div class="entry">
      
        <p><a href="http://www.guokr.com/blog/777525/" target="_blank" rel="external">Nim 游戏</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">传统的Nim游戏是对一些放置成堆的一定数量的硬币开始的：硬币和堆的数量取决于你。</span><br><span class="line">有两名玩家玩这个游戏，当轮到某位玩家时，他能从某一堆里取任意数量的硬币，但是至少要取一枚硬币，但是也不能从除你取的这个堆以外的其他堆里再取硬币。</span><br><span class="line">赢家是取得最后一枚硬币的人，所以最后是没有硬币剩余的。</span><br></pre></td></tr></table></figure>
<p><img src="resource/game/NimGame.jpg" alt="Nim Game"></p>
<p>从这个🌰看 A是最终的胜者！如果我是player，如何才能必胜！！！</p>
<p>两玩家分别称为A和B，A先走。</p>
<p>一共两堆各有一枚硬币。那么显然，玩家B必胜：A必须取两个硬币里的一个，留下一个被B取走。</p>
<p>现在的两堆分别有2和1个硬币。A有必胜策略：从有两个硬币的那堆取一个。这样造成的局面和上面的例子等同，我们知道这时A会赢。</p>
<p>美国数学家Charles Bouton同样感觉到这点并促使他毫不畏惧的开始分析这个游戏。1902年他找到了这个窍门——及其精妙！</p>
<hr>
<p><strong>16个让你烧脑让你晕的悖论</strong></p>
<hr>
<p><strong>最具争议的12个数学事实</strong>  </p>
<p><em>三门问题（Monty Hall problem</em></p>
<p>参赛者会看见三扇关闭了的门，其中一扇的后面有一辆汽车，选中后面有车的那扇门可赢得该汽车，另外两扇门后面则各藏有一只山羊。</p>
<p>当参赛者选定了一扇门，但未去开启它的时候，节目主持人开启剩下两扇门的其中一扇，露出其中一只山羊。</p>
<p>主持人其后会问参赛者要不要换另一扇仍然关上的门。问题是：换另一扇门会否增加参赛者赢得汽车的机会率？</p>
<p>如果严格按照上述的条件，即主持人清楚地知道，哪扇门后是羊，那么答案是会。</p>
<p>不换门的话，赢得汽车的几率是<code>1/3</code>。</p>
<p>换门的话，赢得汽车的几率是<code>2/3</code>。</p>
<hr>
<p><em>0.999…=1</em></p>
<hr>
<p><strong>彭罗斯阶梯(Penrose stairs)</strong></p>
<p>彭罗斯阶梯是一个有名的几何学悖论，指的是一个始终向上或向下但却无限循环的阶梯，可以被视为彭罗斯三角形的一个变体，在此阶梯上永远无法找到最高的一点或者最低的一点。</p>
<p>彭罗斯阶梯不可能在三维空间内存在，但只要放入更高阶的空间，彭罗斯阶梯就可以很容易的实现。如同麦比乌斯圈、克莱因瓶。</p>
<p><img src="resource/game/PenroseStairs.jpg" alt="Penrose stairs"></p>
<hr>
<p>Reference :</p>
<ol>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzI2NjA3NTc4Ng==&amp;mid=2652077912&amp;idx=1&amp;sn=43d9cf785a9470c278c5d011fce9733b&amp;scene=23&amp;srcid=0429cBdkTUxrcr21zeU7L9i7#rd" target="_blank" rel="external">16个让你烧脑让你晕的悖论</a></li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MjM5MTQzNzU2NA==&amp;mid=2651642931&amp;idx=1&amp;sn=b19144eafafb8509f86276ac33a46bca&amp;scene=23&amp;srcid=0724T72b2uKgzYkHuKhI8cuB#rd" target="_blank" rel="external">最具争议的12个数学事实</a></li>
</ol>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-04-13T16:00:00.000Z"><a href="/2016/04/14/live/coffee/">2016-04-14</a></time>
      
      
  
    <h1 class="title"><a href="/2016/04/14/live/coffee/">Coffee</a></h1>
  

    </header>
    <div class="entry">
      
        <h4 id="咖啡萃取的原理-化学反应-物理反应？！"><a href="#咖啡萃取的原理-化学反应-物理反应？！" class="headerlink" title="咖啡萃取的原理=化学反应+物理反应？！"></a><a href="http://www.wtoutiao.com/p/ZfaC0p.html" target="_blank" rel="external">咖啡萃取的原理=化学反应+物理反应？！</a></h4><p>整个咖啡冲泡分成三个阶段：润湿——萃取——水解。</p>
<p>当咖啡粉已经被高温润湿后，气体以及活泼的气体化合物首先逸散，咖啡内部的可溶有机分子同时被渗透压“拉扯”到水中，进而溶解。这个过程主要是咖啡粉与热水接触导致的一系列化学和物理变化，我们称之为“萃取”。<code>但是事实上，这个“萃取”一词也可以与“冲泡”混淆，因为萃取的确是咖啡冲泡中用时最长、影响最大的环节。</code></p>
<hr>
<h4 id="世界各地的人都喝啥样的咖啡"><a href="#世界各地的人都喝啥样的咖啡" class="headerlink" title="世界各地的人都喝啥样的咖啡?"></a><a href="http://www.wtoutiao.com/p/E2d3UJ.html" target="_blank" rel="external">世界各地的人都喝啥样的咖啡?</a></h4><p><code>德国：Pharisäer咖啡</code> 在咖啡里加2盎司朗姆酒。黑咖啡配上朗姆酒和糖，再加上点打发的奶油，味道简直没得说！</p>
<p><code>越南：鸡蛋咖啡</code> 两个蛋黄和炼乳、蜂蜜和香草共同打发，然后再倒入刚刚泡好的越南咖啡里。打发的蛋黄漂浮在咖啡之上，口感丝滑，唇齿留香。</p>
<p><code>美国：Gibraltar咖啡</code> 意大利浓缩咖啡的量加至2杯，配上打发的牛奶</p>
<p><code>西班牙：Cafe Bombón</code> 蜜糖咖啡  由浓缩咖啡和炼乳制成。</p>
<p><code>土耳其：土耳其咖啡</code> 土耳其咖啡壶冲煮研磨极为精细的咖啡粉，口味香甜，口感醇厚。</p>
<p><code>法国：Cafe au Lait</code> 等量的咖啡和打发的牛奶制成。法国人还喜欢在咖啡里加入菊苣，以增加咖啡的甜度。</p>
<p><code>墨西哥：Cafe de Olla</code> 用锅煮的咖啡 将深度烘焙的、研磨好的咖啡粉用煎锅熬制，并加入肉桂棒、橙皮和棕糖。直到棕糖完全融化，关火浸泡5分钟，即可趁热饮用</p>
<p><code>意大利：Romano意大利浓缩咖啡</code> 浓缩咖啡里加入一片柠檬</p>
<p><code>新西兰和澳大利亚：Flat White咖啡</code>  双倍意大利浓缩咖啡和打发至口感细腻的牛奶制成。有人说这种咖啡的口味介于卡布奇诺和拿铁之间。</p>
<p><code>希腊：Frappe咖啡</code> 冰饮咖啡，用速溶咖啡、糖、加水威士忌制成。</p>
<p><code>澳大利亚：Kaisermelange咖啡</code> 皇帝特饮（Emperor’s Blend） 咖啡里加入蛋黄、糖和干邑白兰地</p>
<hr>
<h4 id="咖啡中的酸究竟是什么东西？"><a href="#咖啡中的酸究竟是什么东西？" class="headerlink" title="咖啡中的酸究竟是什么东西？"></a><a href="http://mp.weixin.qq.com/s?__biz=MjM5MTQ3NzE2Mw==&amp;mid=2650452906&amp;idx=2&amp;sn=b8276caa1fb3fcc1a3b1e47b89cd1d65&amp;scene=23&amp;srcid=062550iubemhngkbFlEObyD2#rd" target="_blank" rel="external">咖啡中的酸究竟是什么东西？</a></h4><p>酸是一种化学物质，特征是具有酸味。“酸”在英文中叫做“acid”，拉丁语中意为“sour”。在化学上，酸的水溶液PH值均小于7，且数值越小酸性越大。</p>
<p>很多食物中含有酸性化合物，常见的食物有柠檬，醋，酸奶或咖啡等。单是咖啡，就含有数百种酸性化合物，接下来我们将介绍主要几种影响人类口感的酸。</p>
<ol>
<li>Citric Acid (柠檬酸) : 柠檬酸主要存在于柑橘类水果中，也是大多数蔬菜和水果中常见的酸。柠檬酸是最容易被识别的一种酸性化合物。</li>
<li>Malic Acid (苹果酸) : 青苹果和大黄中含有大量苹果酸。在烹饪界，苹果酸常与青柠檬联系在一起，苹果酸也被认为是一种“未成熟的水果味”。因为随着果实的成熟，其含有的酸度会慢慢降低，所以未成熟的水果或绿葡萄，猕猴桃和醋栗等绿颜色的水果酸度都比较低。</li>
<li>Tartaric Acid (酒石酸) : 葡萄中通常含有较多的酒石酸，酒石是酿酒过程中自然出现的一种物质，其主要成分是酒石酸。酒石还可以用作食品的膨松剂。酒石酸最突出的特点是口感明显。它会让人酸得直流口水，并且它的余韵苦涩。酒石酸也是超酸糖果的主要成分。</li>
<li>Acetic Acid (乙酸) : 乙酸很特别，不仅尝起来很酸，而且闻起来也有一股刺鼻的酸味。浓度低的乙酸会散发令人愉悦的青柠檬味，浓度高的乙酸尝起来和闻起来都有一股发酵的味道。</li>
</ol>
<p>以风味明亮的浅烘焙非洲咖啡豆为例，首先萃取后测试到它的PH值为4.6，可以对应到葡萄，桃子，李子，菠萝等水果风味。接着你在嘴里发现了酒石酸的涩味，也许你会将这个认为是葡萄酸或其他果酸。然后再找到相对应的水果风味。同样，具有柠檬味的低pH值可确定为“柠檬酸”，具有柠檬酸的较高pH值可能为橙子风味。如果pH值较低，苹果酸味道较多可能是青柠檬风味；如果pH值较高，苹果酸味道也较多可能是大黄、青苹果，或葡萄柚的风味。</p>
<hr>
<h4 id="100-vs-36-数量与质量的对决"><a href="#100-vs-36-数量与质量的对决" class="headerlink" title="100 vs 36 | 数量与质量的对决"></a><a href="http://mp.weixin.qq.com/s?__biz=MjM5MTQ3NzE2Mw==&amp;mid=2650453005&amp;idx=1&amp;sn=09ba8b31f8ee99a9756aade2aa1f781c&amp;scene=23&amp;srcid=0628w1S1CodBU58mZv36O1iD#rd" target="_blank" rel="external">100 vs 36 | 数量与质量的对决</a></h4><p>Coffee Aroma Kit T100又称为Coffee Flavor Map(以下简称T100)，在今年2016SCAA展会中首次亮相。SCAA官方网站推荐购买。</p>
<p><code>T100</code>是由Kicci公司旗下的SCENTONE所推出的咖啡风味闻香瓶。正如名字，有着100个风味，并进行了相应的分组。分别是：</p>
<ol>
<li>Tropical Fruit 热带水果类</li>
<li>Berry-Like 浆果类</li>
<li>Citrus &amp; Other Fruits 柑橘类和其他水果类</li>
<li>Stone Fruit 核果类</li>
<li>Cereal &amp; Nut 谷物类和坚果类</li>
<li>Ceramel &amp; Chocolate 焦糖类和巧克力类</li>
<li>Herb &amp; Flower 草本类和花香类</li>
<li>Spice 香料类</li>
<li>Vegetable 蔬菜类</li>
<li>咸味类和其他类</li>
</ol>
<p><code>Le Nez Du Cafe 36</code>，SCAA委托法国香水研发机构Éditions Jean Lenoir设计咖啡风味瓶。老板Jean也设计过红酒专用的酒鼻子Le Nez Du Vin kit，酒鼻子有着56味。</p>
<p>咖啡闻香瓶一直被用于CQI Q-Grader的培训当中，并按照烘焙过程当中的不同阶段的反应进行了分类，分别为4组4个大类，12个小类。</p>
<ol>
<li>Enzymatic 酶促化反应组-Flowery-Fruity-Herbal</li>
<li>Dry Distillation 干馏化反应组 </li>
<li>Sugar Browning 焦糖化反应组 Carmelly-Nutty-Chocolaty</li>
<li>Taints 瑕疵风味/异味组 Earthy-Fermented-Phenoli</li>
</ol>
<hr>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">治疗咖啡因上瘾的唯一方法是提高对咖啡的品鉴能力</span><br><span class="line"></span><br><span class="line">当一个人不再单纯地沉溺于咖啡因带来的神经兴奋而能够感知鞣酸、果香、酒味、苦涩、芳醇等细致的风味时</span><br><span class="line"></span><br><span class="line">他就再也不会对咖啡因上瘾了—— 他会对咖啡本身的味道上瘾</span><br></pre></td></tr></table></figure>
<hr>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-01-21T08:57:31.000Z"><a href="/2016/01/21/distribution/flink/">2016-01-21</a></time>
      
      
  
    <h1 class="title"><a href="/2016/01/21/distribution/flink/"></a></h1>
  

    </header>
    <div class="entry">
      
        <h2 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h2><p>When the Flink system is started, it bring up the JobManager and one or more TaskManagers. </p>
<p>The JobManager is the coordinator of the Flink system, while the TaskManagers are the workers that execute parts of the parallel programs. When starting the system in local mode, a single JobManager and TaskManager are brought up within the same JVM.</p>
<p><img src="../resource/distribution/flink/process_model.svg" alt="flink process model"></p>
<h3 id="Command-Line-Interface"><a href="#Command-Line-Interface" class="headerlink" title="Command Line Interface"></a>Command Line Interface</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line">bin/flink</span><br><span class="line">./flink &lt;ACTION&gt; [OPTIONS] [ARGUMENTS]</span><br><span class="line"></span><br><span class="line">The following actions are available:</span><br><span class="line"></span><br><span class="line">Action &quot;run&quot; compiles and runs a program.</span><br><span class="line"></span><br><span class="line">  Syntax: run [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt;</span><br><span class="line">  &quot;run&quot; action options:</span><br><span class="line">     -c,--class &lt;classname&gt;           Class with the program entry point (&quot;main&quot;</span><br><span class="line">                                      method or &quot;getPlan()&quot; method. Only needed</span><br><span class="line">                                      if the JAR file does not specify the class</span><br><span class="line">                                      in its manifest.</span><br><span class="line">     -m,--jobmanager &lt;host:port&gt;      Address of the JobManager (master) to</span><br><span class="line">                                      which to connect. Specify &apos;yarn-cluster&apos;</span><br><span class="line">                                      as the JobManager to deploy a YARN cluster</span><br><span class="line">                                      for the job. Use this flag to connect to a</span><br><span class="line">                                      different JobManager than the one</span><br><span class="line">                                      specified in the configuration.</span><br><span class="line">     -p,--parallelism &lt;parallelism&gt;   The parallelism with which to run the</span><br><span class="line">                                      program. Optional flag to override the</span><br><span class="line">                                      default value specified in the</span><br><span class="line">                                      configuration.</span><br><span class="line">  Additional arguments if -m yarn-cluster is set:</span><br><span class="line">     -yd,--yarndetached                   Start detached</span><br><span class="line">     -yD &lt;arg&gt;                            Dynamic properties</span><br><span class="line">     -yj,--yarnjar &lt;arg&gt;                  Path to Flink jar file</span><br><span class="line">     -yjm,--yarnjobManagerMemory &lt;arg&gt;    Memory for JobManager Container [in</span><br><span class="line">                                          MB]</span><br><span class="line">     -yn,--yarncontainer &lt;arg&gt;            Number of YARN container to allocate</span><br><span class="line">                                          (=Number of Task Managers)</span><br><span class="line">     -ynm,--yarnname &lt;arg&gt;                Set a custom name for the application</span><br><span class="line">                                          on YARN</span><br><span class="line">     -yq,--yarnquery                      Display available YARN resources</span><br><span class="line">                                          (memory, cores)</span><br><span class="line">     -yqu,--yarnqueue &lt;arg&gt;               Specify YARN queue.</span><br><span class="line">     -ys,--yarnslots &lt;arg&gt;                Number of slots per TaskManager</span><br><span class="line">     -yst,--yarnstreaming                 Start Flink in streaming mode</span><br><span class="line">     -yt,--yarnship &lt;arg&gt;                 Ship files in the specified directory</span><br><span class="line">                                          (t for transfer)</span><br><span class="line">     -ytm,--yarntaskManagerMemory &lt;arg&gt;   Memory per TaskManager Container [in</span><br><span class="line">                                          MB]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Action &quot;info&quot; shows the optimized execution plan of the program (JSON).</span><br><span class="line"></span><br><span class="line">  Syntax: info [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt;</span><br><span class="line">  &quot;info&quot; action options:</span><br><span class="line">     -c,--class &lt;classname&gt;           Class with the program entry point (&quot;main&quot;</span><br><span class="line">                                      method or &quot;getPlan()&quot; method. Only needed</span><br><span class="line">                                      if the JAR file does not specify the class</span><br><span class="line">                                      in its manifest.</span><br><span class="line">     -m,--jobmanager &lt;host:port&gt;      Address of the JobManager (master) to</span><br><span class="line">                                      which to connect. Specify &apos;yarn-cluster&apos;</span><br><span class="line">                                      as the JobManager to deploy a YARN cluster</span><br><span class="line">                                      for the job. Use this flag to connect to a</span><br><span class="line">                                      different JobManager than the one</span><br><span class="line">                                      specified in the configuration.</span><br><span class="line">     -p,--parallelism &lt;parallelism&gt;   The parallelism with which to run the</span><br><span class="line">                                      program. Optional flag to override the</span><br><span class="line">                                      default value specified in the</span><br><span class="line">                                      configuration.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Action &quot;list&quot; lists running and scheduled programs.</span><br><span class="line"></span><br><span class="line">  Syntax: list [OPTIONS]</span><br><span class="line">  &quot;list&quot; action options:</span><br><span class="line">     -m,--jobmanager &lt;host:port&gt;   Address of the JobManager (master) to which</span><br><span class="line">                                   to connect. Specify &apos;yarn-cluster&apos; as the</span><br><span class="line">                                   JobManager to deploy a YARN cluster for the</span><br><span class="line">                                   job. Use this flag to connect to a different</span><br><span class="line">                                   JobManager than the one specified in the</span><br><span class="line">                                   configuration.</span><br><span class="line">     -r,--running                  Show only running programs and their JobIDs</span><br><span class="line">     -s,--scheduled                Show only scheduled programs and their JobIDs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Action &quot;cancel&quot; cancels a running program.</span><br><span class="line"></span><br><span class="line">  Syntax: cancel [OPTIONS] &lt;Job ID&gt;</span><br><span class="line">  &quot;cancel&quot; action options:</span><br><span class="line">     -m,--jobmanager &lt;host:port&gt;   Address of the JobManager (master) to which</span><br><span class="line">                                   to connect. Specify &apos;yarn-cluster&apos; as the</span><br><span class="line">                                   JobManager to deploy a YARN cluster for the</span><br><span class="line">                                   job. Use this flag to connect to a different</span><br><span class="line">                                   JobManager than the one specified in the</span><br><span class="line">                                   configuration.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Please specify an action.</span><br></pre></td></tr></table></figure>
<p>###Configuration</p>
<p>###Web Client<br>Flink provides a web interface to upload jobs, inspect their execution plans, and execute them. </p>
<p>The web interface runs on port 8080 by default.To specify a custom port set the webclient.port property in the conf/flink.yaml.</p>
<p>Jobs are submitted to the JobManager specified by jobmanager.rpc.address and jobmanager.rpc.port.</p>
<p><code>job view</code> Upload a Flink program as a jar file, execute an uploaded program.</p>
<p> jar’s manifest file does not specify the program class, you can specify it before the argument list as:<code>-c &lt;assemblerClass&gt; &lt;programArgs...&gt;</code></p>
<p><code>Plan View</code>  optimized execution plan of the submitted program in the upper half of the page.</p>
<h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>The batch processing APIs of Flink are centered around the DataSet abstraction. </p>
<p>A DataSet is only an abstract representation of a set of data that can contain duplicates.</p>
<table>
<thead>
<tr>
<th>Transformation</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Map</td>
<td></td>
</tr>
<tr>
<td>FlatMap</td>
<td></td>
</tr>
<tr>
<td>MapPartition</td>
<td></td>
</tr>
<tr>
<td>Filter</td>
<td></td>
</tr>
<tr>
<td>Reduce</td>
<td></td>
</tr>
<tr>
<td>ReduceGroup</td>
<td></td>
</tr>
<tr>
<td>Aggregate</td>
<td></td>
</tr>
<tr>
<td>Join</td>
<td></td>
</tr>
<tr>
<td>CoGroup</td>
<td></td>
</tr>
<tr>
<td>Cross</td>
<td></td>
</tr>
<tr>
<td>Union</td>
<td></td>
</tr>
<tr>
<td>Rebalance</td>
<td></td>
</tr>
<tr>
<td>Hash-Partition</td>
<td></td>
</tr>
<tr>
<td>Custom Partitioning</td>
<td></td>
</tr>
<tr>
<td>Sort Partition</td>
<td></td>
</tr>
<tr>
<td>First-n</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="Program-Skeleton"><a href="#Program-Skeleton" class="headerlink" title="Program Skeleton"></a>Program Skeleton</h3><p>Flink programs look like regular Java programs with a main() method. Each program consists of the same basic parts:</p>
<ol>
<li>Obtain an ExecutionEnvironment</li>
<li>Load/create the initial data</li>
<li>Specify transformations on this data</li>
<li>Specify where to put the results</li>
<li>Trigger the program execution</li>
</ol>
<h3 id="DataSet-Transformations"><a href="#DataSet-Transformations" class="headerlink" title="DataSet Transformations"></a>DataSet Transformations</h3><h5 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h5><p>Applies a map function on each element of a DataSet.</p>
<p>One element must be returned by the function.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.flink.api.common.functions.MapFunction;</span><br><span class="line">import org.apache.flink.api.java.tuple.Tuple2;</span><br><span class="line"></span><br><span class="line">public class SumMapFunction implements MapFunction&lt;Tuple2&lt;Integer, Integer&gt;, Integer&gt; &#123;</span><br><span class="line"></span><br><span class="line">	private static final long serialVersionUID = 3830811031233724943L;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public Integer map(Tuple2&lt;Integer, Integer&gt; tuple) throws Exception &#123;</span><br><span class="line">		return tuple.f0 + tuple.f1;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="FlatMap"><a href="#FlatMap" class="headerlink" title="FlatMap"></a>FlatMap</h5><p>Applies a flat-map function on each element of a DataSet. </p>
<p>This variant of a map function can return arbitrary many result elements (including none) for each input element.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.flink.api.common.functions.FlatMapFunction;</span><br><span class="line">import org.apache.flink.util.Collector;</span><br><span class="line"></span><br><span class="line">public class Tokenizer implements FlatMapFunction&lt;String, String&gt; &#123;</span><br><span class="line"></span><br><span class="line">	private static final long serialVersionUID = 2350322299357354023L;</span><br><span class="line"></span><br><span class="line">	@Override</span><br><span class="line">	public void flatMap(String line, Collector&lt;String&gt; collector) throws Exception &#123;</span><br><span class="line">		for (String token : line.split(&quot;\\W&quot;)) &#123;</span><br><span class="line">			collector.collect(token);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="MapPartition"><a href="#MapPartition" class="headerlink" title="MapPartition"></a>MapPartition</h5><p>Transforms a parallel partition in a single function call. </p>
<p>The map-partition function gets the partition as Iterable and can produce an arbitrary number of result values. </p>
<p>The number of elements in each partition depends on the degree-of-parallelism and previous operations.</p>
<h5 id="Filter"><a href="#Filter" class="headerlink" title="Filter"></a>Filter</h5><h5 id="Project"><a href="#Project" class="headerlink" title="Project"></a>Project</h5><h5 id="Grouped-DataSet"><a href="#Grouped-DataSet" class="headerlink" title="Grouped DataSet"></a>Grouped DataSet</h5><h5 id="Combinable-GroupReduceFunctions"><a href="#Combinable-GroupReduceFunctions" class="headerlink" title="Combinable GroupReduceFunctions"></a>Combinable GroupReduceFunctions</h5><h5 id="GroupCombine-on-a-Grouped-DataSet"><a href="#GroupCombine-on-a-Grouped-DataSet" class="headerlink" title="GroupCombine on a Grouped DataSet"></a>GroupCombine on a Grouped DataSet</h5><h5 id="Aggregate-on-Grouped-Tuple-DataSet"><a href="#Aggregate-on-Grouped-Tuple-DataSet" class="headerlink" title="Aggregate on Grouped Tuple DataSet"></a>Aggregate on Grouped Tuple DataSet</h5><h5 id="Reduce-on-full-DataSet"><a href="#Reduce-on-full-DataSet" class="headerlink" title="Reduce on full DataSet"></a>Reduce on full DataSet</h5><h5 id="GroupReduce-on-full-DataSet"><a href="#GroupReduce-on-full-DataSet" class="headerlink" title="GroupReduce on full DataSet"></a>GroupReduce on full DataSet</h5><h5 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h5><h5 id="Cross"><a href="#Cross" class="headerlink" title="Cross"></a>Cross</h5><h5 id="CoGroup"><a href="#CoGroup" class="headerlink" title="CoGroup"></a>CoGroup</h5><h5 id="Union"><a href="#Union" class="headerlink" title="Union"></a>Union</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1.union(vals2).union(vals3);</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1.union(vals2).union(vals3);</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1.union(vals2).union(vals3);</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1.union(vals2).union(vals3);DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1.union(vals2).union(vals3);DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1.union(vals2).union(vals3);DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals1 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals2 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; vals3 = // [...]</span><br><span class="line">DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; unioned = vals1.union(vals2).union(vals3);</span><br></pre></td></tr></table></figure>
<h5 id="First-n"><a href="#First-n" class="headerlink" title="First-n"></a>First-n</h5><h3 id="Flink-Streaming"><a href="#Flink-Streaming" class="headerlink" title="Flink Streaming"></a>Flink Streaming</h3><p><code>Transformations</code></p>
<p>Data transformations transform one or more DataStreams into a new DataStream.</p>
<table>
<thead>
<tr>
<th>Transformation</th>
<th>Structure</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Map</td>
<td>DataStream → DataStream</td>
<td>Takes one element and produces one element.</td>
<td>dataStream.map { x =&gt; x * 2 }</td>
</tr>
<tr>
<td>FlatMap</td>
<td>DataStream → DataStream</td>
<td>Takes one element and produces zero, one, or more elements.</td>
<td>dataStream.flatMap { str =&gt; str.split(“ “) }</td>
</tr>
<tr>
<td>Filter</td>
<td>DataStream → DataStream</td>
<td>Evaluates a boolean function for each element and retains those for which the function returns true.</td>
<td>dataStream.filter { _ != 0 }</td>
</tr>
<tr>
<td>KeyBy</td>
<td>DataStream → KeyedStream</td>
<td>Logically partitions a stream into disjoint partitions, each partition containing elements of the same key.</td>
<td>dataStream.keyBy(“someKey”) dataStream.keyBy(0) </td>
</tr>
<tr>
<td>Reduce</td>
<td>KeyedStream → DataStream</td>
<td></td>
<td>keyedStream.reduce { <em> + </em> }</td>
</tr>
<tr>
<td>Fold</td>
<td>KeyedStream → DataStream</td>
<td></td>
<td>val result: DataStream[String] = keyedStream.fold(“start”, (str, i) =&gt; { str + “-“ + i })</td>
</tr>
<tr>
<td>Aggregations</td>
<td>KeyedStream → DataStream</td>
<td></td>
<td>keyedStream.sum(0) keyedStream.sum(“key”) keyedStream.min(0) keyedStream.min(“key”) keyedStream.max(0) keyedStream.max(“key”) keyedStream.minBy(0) keyedStream.minBy(“key”) keyedStream.maxBy(0) keyedStream.maxBy(“key”)</td>
</tr>
<tr>
<td>Window</td>
<td>KeyedStream → WindowedStream</td>
<td></td>
<td>dataStream.keyBy(0).window(TumblingTimeWindows.of(Time.of(5, TimeUnit.SECONDS)))</td>
</tr>
<tr>
<td>WindowAll</td>
<td>DataStream → AllWindowedStream</td>
<td></td>
<td>dataStream.windowAll(TumblingTimeWindows.of(Time.of(5, TimeUnit.SECONDS)))</td>
</tr>
<tr>
<td>Window Apply</td>
<td>WindowedStream → DataStream AllWindowedStream → DataStream</td>
<td></td>
<td>windowedStream.apply { applyFunction }</td>
</tr>
<tr>
<td>Window Reduce</td>
<td>WindowedStream → DataStream</td>
<td></td>
<td>windowedStream.reduce { <em> + </em> }</td>
</tr>
<tr>
<td>Window Fold</td>
<td>WindowedStream → DataStream</td>
<td></td>
<td>val result: DataStream[String] = windowedStream.fold(“start”, (str, i) =&gt; { str + “-“ + i })</td>
</tr>
<tr>
<td>Aggregations on windows</td>
<td>WindowedStream → DataStream</td>
<td></td>
<td>windowedStream.sum(0) windowedStream.sum(“key”) windowedStream.min(0) windowedStream.min(“key”) windowedStream.max(0) windowedStream.max(“key”) windowedStream.minBy(0) windowedStream.minBy(“key”) windowedStream.maxBy(0) windowedStream.maxBy(“key”)</td>
</tr>
<tr>
<td>Union</td>
<td>DataStream* → DataStream</td>
<td></td>
<td>dataStream.union(otherStream1, otherStream2, …)</td>
</tr>
<tr>
<td>Window Join</td>
<td>DataStream,DataStream → DataStream</td>
<td></td>
<td>dataStream.join(otherStream).where(0).equalTo(1).window(TumblingTimeWindows.of(Time.of(3, TimeUnit.SECONDS))).apply { … }</td>
</tr>
<tr>
<td>Window CoGroup</td>
<td>DataStream,DataStream → DataStream</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Connect</td>
<td>DataStream,DataStream → ConnectedStreams</td>
<td></td>
<td></td>
</tr>
<tr>
<td>CoMap, CoFlatMap</td>
<td>ConnectedStreams → DataStream</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Split</td>
<td>DataStream → SplitStream</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Select</td>
<td>SplitStream → DataStream</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Iterate</td>
<td>DataStream → IterativeStream → DataStream</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Extract Timestamps</td>
<td>DataStream → DataStream</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h4 id="Table-API-Relational-Queries"><a href="#Table-API-Relational-Queries" class="headerlink" title="Table API - Relational Queries"></a>Table API - Relational Queries</h4><p>Flink provides an API that allows specifying operations using SQL-like expressions.</p>
<p>Reference :</p>
<ol>
<li><a href="https://flink.apache.org/news/2015/02/09/streaming-example.html" target="_blank" rel="external">Introducing Flink Streaming</a></li>
</ol>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-01-21T08:57:31.000Z"><a href="/2016/01/21/distribution/flume/">2016-01-21</a></time>
      
      
  
    <h1 class="title"><a href="/2016/01/21/distribution/flume/"></a></h1>
  

    </header>
    <div class="entry">
      
        <p>###Flume</p>
<p>#####Source<br><code>Source</code> is to receive data from an external client and store it into the configured Channels. A Source can get an instance of its own ChannelProcessor to process an Event, commited within a Channel local transaction, in serial. In the case of an exception, required Channels will propagate the exception, all Channels will rollback their transaction, but events processed previously on other Channels will remain committed.</p>
<p>Similar to the SinkRunner.PollingRunner Runnable, there’s a PollingRunner Runnable that executes on a thread created when the Flume framework calls PollableSourceRunner.start(). Each configured PollableSource is associated with its own thread that runs a PollingRunner. This thread manages the PollableSource’s lifecycle, such as starting and stopping. A PollableSource implementation must implement the start() and stop() methods that are declared in the LifecycleAware interface. The runner of a PollableSource invokes that Source‘s process() method. The process() method should check for new data and store it into the Channel as Flume Events.</p>
<p>Note that there are actually two types of Sources. The PollableSource was already mentioned. The other is the EventDrivenSource. The EventDrivenSource, unlike the PollableSource, must have its own callback mechanism that captures the new data and stores it into the Channel. The EventDrivenSources are not each driven by their own thread like the PollableSources are. Below is an example of a custom PollableSource:</p>
<p>#####Channel</p>
<p>#####Sink</p>
<p>#####Exceptions Collection</p>
<ol>
<li><code>Space for commit to queue couldn&#39;t be acquired Sinks are likely not keeping up with sources, or the buffer size is too tight</code></li>
</ol>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2016-01-21T08:57:31.000Z"><a href="/2016/01/21/distribution/gearpump/">2016-01-21</a></time>
      
      
  
    <h1 class="title"><a href="/2016/01/21/distribution/gearpump/"></a></h1>
  

    </header>
    <div class="entry">
      
        <p>####Gearpump</p>
<p>#####Basic Concepts</p>
<ol>
<li><p>System timestamp and Application timestamp</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   System timestamp is the time of backend cluster system. </span><br><span class="line">   Application timestamp is the time at which message generated.</span><br><span class="line">   ``` </span><br><span class="line"></span><br><span class="line">2. Master, and Worker</span><br></pre></td></tr></table></figure>
<p> Gearpump follow master slave architecture.<br> Every cluster contains one or more Master node, and several worker nodes.<br> Worker node is responsible to manage local resources on single machine.<br> Master node is responsible to manage global resources of the whole cluster.    </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> </span><br><span class="line"></span><br><span class="line">3. Application</span><br></pre></td></tr></table></figure>
<p>Gearpump natively supports Streaming Application types</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">   </span><br><span class="line">4. AppMaster and Executor</span><br></pre></td></tr></table></figure>
</li>
</ol>
   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">   </span><br><span class="line">5. Application Submission Flow</span><br></pre></td></tr></table></figure>
   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">   </span><br><span class="line">6. Streaming Topology, Processor, and Task</span><br></pre></td></tr></table></figure>
<p>   Each application contains a topology, which is a DAG to describe the data flow.<br>   Each node in the DAG is a processor.<br>   An application is a DAG of processors. Each processor handles messages.<br>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">   ![Directed Acyclic Graph](../resource/distribution/gearpump/dag.png)   </span><br><span class="line">   </span><br><span class="line">7. Streaming Task and Partitioner</span><br></pre></td></tr></table></figure></p>
<p>   Task is the minimum unit of parallelism.<br>   ``` </p>
<p>#####Command Line Interface</p>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-12-31T16:00:00.000Z"><a href="/2016/01/01/language/clojure/">2016-01-01</a></time>
      
      
  
    <h1 class="title"><a href="/2016/01/01/language/clojure/">Clojur</a></h1>
  

    </header>
    <div class="entry">
      
        <h3 id="Clojur"><a href="#Clojur" class="headerlink" title="Clojur"></a>Clojur</h3>
      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>




  <article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-12-31T16:00:00.000Z"><a href="/2016/01/01/distribution/spark/">2016-01-01</a></time>
      
      
  
    <h1 class="title"><a href="/2016/01/01/distribution/spark/">spark</a></h1>
  

    </header>
    <div class="entry">
      
        <h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><p>Apache Spark is a fast and general-purpose cluster computing system. </p>
<p>It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. </p>
<p>It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.</p>
<p>Spark’s primary abstraction is a distributed collection of items called a Resilient Distributed Dataset (RDD). RDDs can be created from Hadoop InputFormats or by transforming other RDDs. </p>
<p>RDDs have actions, which return values, and transformations, which return pointers to new RDDs.</p>
<p><img src="resource/spark/architecture.jpg" alt="architecture"></p>
<h3 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h3><p>One of the strongest features of Spark is its shell. </p>
<p>The Spark-Shell allows users to type and execute commands in a Unix-Terminal-like fashion. </p>
<ol>
<li><p>To adjust the amount of memory that Spark may use for executing queries you have to set the following environment prior to starting the shell <code>export SPARK_MEM =1 g</code></p>
</li>
<li><p>To  controls the number of worker threads that Spark uses <code>export SPARK_WORKER_INSTANCES =4</code> If you run Spark in local mode you can also set the number of worker threads in one setting as follows: <code>export MASTER = local [32]</code></p>
</li>
</ol>
<hr>
<h3 id="RDD-API"><a href="#RDD-API" class="headerlink" title="RDD API"></a>RDD API</h3><p>RDD is short for Resilient Distributed Dataset. RDDs are the workhorse of the Spark<br>system. As a user, one can consider a RDD as a handle for a collection of individual data partitions which are the result of some computation.</p>
<p>RDDs have <strong>actions</strong>, which return values, and <strong>transformations</strong>, which return pointers to new RDDs.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">λ ~/service/spark-1.6.0/ bin/spark-shell</span><br><span class="line">log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).</span><br><span class="line">log4j:WARN Please initialize the log4j system properly.</span><br><span class="line">log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span><br><span class="line">Using Spark&apos;s repl log4j profile: org/apache/spark/log4j-defaults-repl.properties</span><br><span class="line">To adjust logging level use sc.setLogLevel(&quot;INFO&quot;)</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</span><br><span class="line">Type in expressions to have them evaluated.</span><br><span class="line">Type :help for more information.</span><br><span class="line">16/01/24 21:53:16 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.1.142 instead (on interface en0)</span><br><span class="line">16/01/24 21:53:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address</span><br><span class="line">Spark context available as sc.</span><br><span class="line">SQL context available as sqlContext.</span><br><span class="line"></span><br><span class="line">scala&gt; val list = sc.parallelize(List(1,2,3,4,5,6), 2)</span><br><span class="line">list: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:27</span><br></pre></td></tr></table></figure>
<p><strong>Spark会把数据都载入到内存么 ? </strong></p>
<p><em>如果没有主动对RDD进行Cache/Persist等相关操作，RDD不过是一个概念上存在的虚拟数据集</em></p>
<p>一个 RDD 本质上是一个函数，RDD变换不过是函数嵌套。 RDD有两类：</p>
<ol>
<li>输入RDD,典型如KafkaRDD，JdbcRDD以及HadoopRDD等</li>
<li>转换RDD，如MapPartitionsRDD</li>
</ol>
<p><strong>Shuffle的本质是什么 ? </strong></p>
<hr>
<h3 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h3><p><code>Spark SQL</code>核心是把已有RDD，带上Schema信息，注册成类似SQL里的<code>Table</code>，对其进行SQL查询。这里面主要分两部分，一是生成SchemaRD，二是执行查询。</p>
<p>传统数据库先将读入的SQL语句进行解析，分辨出SQL语句中哪些词是关键字，哪些是表达式，哪些是Projection，哪些是Data Source等等。</p>
<p>进一步判断SQL语句是否规范，不规范就报错，规范则按照下一步过程绑定。</p>
<p>过程绑定是将SQL语句和数据库的数据字典(列，表，视图等）进行绑定，如果相关的Projection、Data Source等都存在，就表示这个SQL语句是可以执行的。</p>
<p>在执行过程中，有时候甚至不需要读取物理表就可以返回结果，比如重新运行刚运行过的SQL语句，直接从数据库的缓冲池中获取返回结果。</p>
<p>在数据库解析的过程中SQL语句时，将会把SQL语句转化成一个树形结构来进行处理，会形成一个或含有多个节点的Tree，然后再后续的处理政对该Tree进行一系列的操作。</p>
<p>Spark SQL对SQL语句的处理和关系数据库对SQL语句的解析采用了类似的方法，首先会将SQL语句进行解析，然后形成一个Tree，后续如绑定、优化等处理过程都是对Tree的操作，而操作方法是采用Rule，通过模式匹配，对不同类型的节点采用不同的操作。SparkSQL有两个分支，SQLContext和HiveContext。</p>
<p>SQLContext现在只支持SQL语法解析器（Catalyst)，HiveContext支持SQL语法和HiveContext语法解析器。</p>
<p>Spark SQL is a Spark module for structured data processing. </p>
<p>It provides a programming abstraction called DataFrames and can also act as distributed SQL query engine.</p>
<ol>
<li>DataFrame: a distributed collection of data organized into named columns.</li>
</ol>
<p><code>Spark SQL</code> 对 SQL 语句的处理和关系型数据库对 SQL 语句的处理采用了类似的方法。首先会将 SQL 语句进行解析(Parse),然后形成一个 Tree,在后续的如绑定、优化等处 理过程都是对 Tree 的操作,而操作的方法是采用 Rule,通过模式匹配,对不同类型的节点 采用不同的操作。</p>
<p><img src="resource/spark/SparkSQLOverview.png" alt="SparkSQL-Overview"></p>
<p><code>Tree</code></p>
<p><code>TreeNode</code> 可以细分成三种类型的 Node : </p>
<ol>
<li>UnaryNode 一元节点,即只有一个子节点。</li>
<li>BinaryNode 二元节点,即有左右子节点的二叉节点。</li>
<li>LeafNode 叶子节点,没有子节点的节点。</li>
</ol>
<p><code>Rule</code> </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">         SQL Parser                          Analyer                        Optimizer</span><br><span class="line">SQL Text ----------&gt; Unresolved Logical Plan -------&gt; Resolved Logical Plan ---------&gt;</span><br><span class="line"></span><br><span class="line">                       Spark Plan                 prepare for execution                             execute</span><br><span class="line">Optimized Logical Plan ----------&gt; Physical Plan ----------------------&gt;  Executable Physical Plan --------&gt;</span><br><span class="line"></span><br><span class="line">         toRDD</span><br><span class="line">Execute -------&gt; SchemaRDD</span><br></pre></td></tr></table></figure>
<p><code>Catalyst</code> </p>
<ol>
<li>SQL Parse 完成 SQL 语句语法解析功能</li>
<li>Analyzer 主要完成绑定工作,将不同来源的 Unresolved LogicalPlan 和数据元数据进行绑定,生成 resolved LogicalPlan。</li>
<li>Optimizer 对 Resolved LogicalPlan 进行优化,生成 Optimized LogicalPlan。</li>
<li>Planner 将 LogicalPlan 转换成 PhysicalPlan。</li>
<li>CostModel 主要根据过去的性能统计数据,选择最佳的物理执行计划。</li>
</ol>
<hr>
<h3 id="Streaming"><a href="#Streaming" class="headerlink" title="Streaming"></a>Streaming</h3><p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.</p>
<p>Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data.</p>
<p>Internally, a DStream is represented as a sequence of RDDs.</p>
<p><img src="resource/spark/streaming-dstream-window.png" alt="streaming-dstream-window"></p>
<p><img src="resource/spark/words-dstream.png" alt="streaming-dstream-window"></p>
<p><img src="resource/spark/lines-dstream.png" alt="streaming-dstream-window"></p>
<hr>
<h3 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h3><p>GraphX是一个新的 Spark API，它用于图和并行图计算。GraphX通过引入<code>Resilient Distributed Property Graph</code> (带有顶点和边属性有向多重图)，来扩展Spark RDD。为了支持图计算，GraphX公开一组基本的功能操作以及Pregel API的优化。</p>
<p>从社交网络到语言建模，不断增长的规模和图形数据的重要性已经推动了许多新的graph-parallel系统（如Giraph和GraphLab）的发展。通过限制可表达的计算类型和引入新的技术来划分和分配图，这些系统可以高效地执行复杂的图形算法，比一般的data-parallel系统快很多。</p>
<p><img src="resource/spark/GraphX.png" alt="GraphX"></p>
<p><code>属性图</code>是一个有向多重图，它带有连接到每个顶点和边的用户定义的对象。有向多重图中多个并行(parallel)的边共享相同的源和目的地顶点。</p>
<p>支持并行边的能力简化了建模场景，这个场景中，相同的顶点存在多种关系。每个顶点由一个唯一的64位长的标识符（VertexID）作为key。GraphX并没有对顶点标识强加任何排序。</p>
<p>同样，顶点拥有相应的源和目的顶点标识符。</p>
<p>属性图通过vertex(VD)和edge(ED)类型参数化，这些类型是分别与每个顶点和边相关联的对象的类型。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">val vertexRDD: RDD[(Long, (String, Int))] = sc.parallelize(vertexArray)</span><br><span class="line">val edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArray)</span><br><span class="line"></span><br><span class="line">val vertexArray = Array(</span><br><span class="line">  (1L, (&quot;Alice&quot;, 28)),</span><br><span class="line">  (2L, (&quot;Bob&quot;, 27)),</span><br><span class="line">  (3L, (&quot;Charlie&quot;, 65)),</span><br><span class="line">  (4L, (&quot;David&quot;, 42)),</span><br><span class="line">  (5L, (&quot;Ed&quot;, 55)),</span><br><span class="line">  (6L, (&quot;Fran&quot;, 50)))</span><br><span class="line">  </span><br><span class="line">val edgeArray = Array(</span><br><span class="line">  Edge(2L, 1L, 7),</span><br><span class="line">  Edge(2L, 4L, 2),</span><br><span class="line">  Edge(3L, 2L, 4),</span><br><span class="line">  Edge(3L, 6L, 3),</span><br><span class="line">  Edge(4L, 1L, 1),</span><br><span class="line">  Edge(5L, 2L, 2),</span><br><span class="line">  Edge(5L, 3L, 8),</span><br><span class="line">  Edge(5L, 6L, 3))</span><br><span class="line"></span><br><span class="line">val vertexRDD: RDD[(Long, (String, Int))] = sc.parallelize(vertexArray)</span><br><span class="line">val edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArray)</span><br><span class="line"></span><br><span class="line">val graph: Graph[(String, Int), Int] = Graph(vertexRDD, edgeRDD)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="MLlib"><a href="#MLlib" class="headerlink" title="MLlib"></a>MLlib</h3><p>MLlib 是 Spark 对常用的机器学习算法的实现库,同时包括相关的测试和数据生成器。</p>
<p>Spark 的设计初衷就是为了支持一些迭代的 Job, 这正好是很多机器学习算法的性质。</p>
<p>MLlib 目前支持四种常见的机器学习问题: 分类, 回归, 聚类和协同过滤。</p>
<hr>
<h3 id="R"><a href="#R" class="headerlink" title="R"></a>R</h3><p>SparkR provides a distributed data frame implementation that supports operations like selection, filtering, aggregation etc.</p>
<p>SparkR also supports distributed machine learning using MLlib.</p>
<hr>
<h3 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h3><p>There are two deploy modes that can be used to launch Spark applications on YARN.</p>
<p><strong>In cluster mode</strong>, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. </p>
<p><strong>In client mode</strong>, the driver runs in the client process, and the application master is only used for requesting resources from YARN.</p>
<p>In YARN mode the ResourceManager’s address is picked up from the Hadoop configuration. Thus, the <code>--master</code> parameter is yarn.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] &lt;app jar&gt; [app options]</span><br></pre></td></tr></table></figure>
<p><code>客户端进行操作</code></p>
<ol>
<li>根据yarnConf来初始化yarnClient，并启动yarnClient</li>
<li>创建客户端Application，并获取Application的ID，进一步判断集群中的资源是否满足executor和ApplicationMaster申请的资源，如果不满足则抛出IllegalArgumentException</li>
<li>设置资源、环境变量：其中包括了设置Application的Staging目录、准备本地资源、设置Application其中的环境变量、创建Container启动的Context等</li>
<li>设置Application提交的Context，包括设置应用的名字、队列、AM的申请的Container、标记该作业的类型为Spark</li>
<li>申请Memory，并最终通过yarnClient.submitApplication向ResourceManager提交该Application</li>
</ol>
<p><code>提交到YARN集群</code></p>
<ol>
<li>运行ApplicationMaster的run方法</li>
<li>设置好相关的环境变量</li>
<li>创建amClient，并启动</li>
<li>在Spark UI启动之前设置Spark UI的AmIpFilter</li>
<li>在startUserClass函数专门启动了一个线程来启动用户提交的Application，也就是启动了Driver。在Driver中将会初始化SparkContext</li>
<li>等待SparkContext初始化完成，最多等待spark.yarn.applicationMaster.waitTries次数，如果等待了的次数超过了配置的，程序将会退出；否则用SparkContext初始化yarnAllocator</li>
<li>当SparkContext、Driver初始化完成的时候，通过amClient向ResourceManager注册ApplicationMaster</li>
<li>分配并启动Executeors。在启动Executeors之前，先要通过yarnAllocator获取到numExecutors个Container，然后在Container中启动Executeors</li>
<li>最后，Task将在oarseGrainedExecutorBackend里面运行，然后运行状况会通过Akka通知CoarseGrainedScheduler，直到作业运行完成</li>
</ol>
<hr>
<p><strong>Config</strong></p>
<p><strong>Application Properties</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.app.name</td>
<td>The name of your application.</td>
</tr>
<tr>
<td>spark.driver.cores</td>
<td>Number of cores to use for the driver process (only cluster mode).</td>
</tr>
<tr>
<td>spark.driver.maxResultSize</td>
<td>Limit of total size of serialized results of all partitions for each Spark action.</td>
</tr>
<tr>
<td>spark.driver.memory</td>
<td>Memory to use for the driver process</td>
</tr>
<tr>
<td>spark.executor.memory</td>
<td>Memory to use per executor process</td>
</tr>
<tr>
<td>spark.extraListeners</td>
<td></td>
</tr>
<tr>
<td>spark.local.dir</td>
<td></td>
</tr>
<tr>
<td>spark.logConf</td>
<td></td>
</tr>
<tr>
<td>spark.master</td>
<td>The cluster manager to connect </td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.yarn.am.memory</td>
<td>Amount of memory to use for the YARN Application Master in client mode</td>
</tr>
<tr>
<td>spark.driver.cores</td>
<td>Number of cores used by the driver in YARN cluster mode.</td>
</tr>
<tr>
<td>spark.yarn.am.cores</td>
<td>Number of cores to use for the YARN Application Master in client mode.</td>
</tr>
<tr>
<td>spark.yarn.am.waitTime</td>
<td>In cluster mode, time for the YARN Application Master to wait for the SparkContext to be initialized. In client mode, time for the YARN Application Master to wait for the driver to connect to it.</td>
</tr>
<tr>
<td>spark.yarn.submit.file.replication</td>
<td>HDFS replication level for the files uploaded into HDFS. </td>
</tr>
<tr>
<td>spark.yarn.preserve.staging.files</td>
<td>Set to true to preserve the staged files at the end of the job.</td>
</tr>
<tr>
<td>spark.yarn.scheduler.heartbeat.interval-ms</td>
<td>Spark application master heartbeats into the YARN ResourceManager interval.</td>
</tr>
<tr>
<td>spark.yarn.scheduler.initial-allocation.interval</td>
<td>The initial interval in which the Spark application master eagerly heartbeats to the YARN ResourceManager when there are pending container allocation requests. </td>
</tr>
<tr>
<td>spark.yarn.max.executor.failures</td>
<td>The maximum number of executor failures before failing the application.</td>
</tr>
<tr>
<td>spark.yarn.historyServer.address</td>
<td>The address of the Spark history server</td>
</tr>
<tr>
<td>spark.yarn.dist.archives</td>
<td>Comma separated list of archives to be extracted into the working directory of each executor.</td>
</tr>
<tr>
<td>spark.yarn.dist.files</td>
<td>Comma-separated list of files to be placed in the working directory of each executor.</td>
</tr>
<tr>
<td>spark.executor.instances</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.executor.memoryOverhead</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.driver.memoryOverhead</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.memoryOverhead</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.port</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.queue</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.jar</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.access.namenodes</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.appMasterEnv.[EnvironmentVariableName]</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.containerLauncherMaxThreads</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.extraJavaOptions</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.extraLibraryPath</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.maxAppAttempts</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.attemptFailuresValidityInterval</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.submit.waitAppCompletion</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.nodeLabelExpression</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.executor.nodeLabelExpression</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.tags</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.keytab</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.principal</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.config.gatewayPath</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.config.replacementPath</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.security.tokens.${service}.enabled</td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Shuffle Behavior</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.reducer.maxSizeInFlight</td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Spark UI</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Compression and Serialization</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Memory Management</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Execution Behavior</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><img src="resource/spark//spark_on_yarn.jpg" alt="spark_on_yarn"></p>
<hr>
<h3 id="PySpark"><a href="#PySpark" class="headerlink" title="PySpark"></a>PySpark</h3><h3 id="Config"><a href="#Config" class="headerlink" title="Config"></a>Config</h3><table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.app.name</td>
<td>The name of your application. </td>
</tr>
<tr>
<td>spark.driver.cores</td>
<td>Number of cores to use for the driver process, only in cluster mode.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>Spark applications run as independent sets of processes on a cluster,coordinated by the SparkContext (<code>driver program</code>).</p>
<p>SparkContext can connect to several types of cluster managers.</p>
<p><img src="resource/spark/cluster-overview.png" alt="architecture"></p>
<table>
<thead>
<tr>
<th>Term</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Application</td>
<td>User program built on Spark.</td>
</tr>
<tr>
<td>Driver program</td>
<td>The process running the main() function of the application and creating the SparkContext</td>
</tr>
<tr>
<td>Cluster manager</td>
<td>An external service for acquiring resources on the cluster</td>
</tr>
<tr>
<td>Deploy mode</td>
<td>Distinguishes where the driver process runs.</td>
</tr>
<tr>
<td>Worker node</td>
<td>Any node that can run application code in the cluster</td>
</tr>
<tr>
<td>Executor</td>
<td>A process that runs tasks and keeps data in memory or disk storage.</td>
</tr>
<tr>
<td>Task</td>
<td>A unit of work that will be sent to one executor</td>
</tr>
<tr>
<td>Job</td>
<td>A parallel computation consisting of multiple tasks</td>
</tr>
<tr>
<td>Stage</td>
<td>Each job gets divided into smaller sets of tasks</td>
</tr>
</tbody>
</table>
<p><code>Spark Driver 是执行main()方法的进程</code></p>
<p><code>执行器节点</code></p>
<h3 id="DataFrame-amp-DataSet"><a href="#DataFrame-amp-DataSet" class="headerlink" title="DataFrame &amp; DataSet"></a>DataFrame &amp; DataSet</h3><p>Reference:</p>
<ol>
<li><a href="http://www.infoq.com/cn/articles/2015-Review-Spark" target="_blank" rel="external">解读2015之Spark篇：新生态系统的形成</a></li>
<li><a href="http://www.infoq.com/cn/presentations/gc-tuning-of-spark-application?utm_campaign=rightbar_v2&amp;utm_source=infoq&amp;utm_medium=presentations_link&amp;utm_content=link_text" target="_blank" rel="external">Spark应用的GC调优</a></li>
<li><a href="http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html" target="_blank" rel="external">The RDD API By Example</a></li>
<li><a href="https://databricks.com/blog" target="_blank" rel="external">Databricks Blog</a></li>
<li><a href="https://databricks.gitbooks.io/databricks-spark-reference-applications/content/index.html" target="_blank" rel="external">databricks.gitbooks.io</a></li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzA5MTc0NTMwNQ==&amp;mid=402777570&amp;idx=1&amp;sn=b06881f5d374cc181784cd8ba31893ad&amp;scene=23&amp;srcid=0318eIjnKdNTXNnQxbXTVpyp#rd" target="_blank" rel="external">一个SparkSQL作业的一生</a></li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzA5MTc0NTMwNQ==&amp;mid=402803628&amp;idx=1&amp;sn=bd72f7e43ddefb0946121778ac161ab5&amp;scene=23&amp;srcid=0318z3Rle2LCaZomw5S45p9D#rd" target="_blank" rel="external">Spark生态顶级项目汇总</a></li>
<li><a href="http://ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html" target="_blank" rel="external">graph analytics with graphx</a></li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzA5MTc0NTMwNQ==&amp;mid=2650713520&amp;idx=1&amp;sn=9e27441611ac6411eca5c13b489a6d37&amp;scene=23&amp;srcid=0421d2KxIwEnxywTxa2W06TT#rd" target="_blank" rel="external">Spark会把数据都载入到内存么</a></li>
<li><a href="http://www.kancloud.cn/kancloud/spark-programming-guide/51557" target="_blank" rel="external">GraphX编程指南</a></li>
<li><a href="https://snap.stanford.edu/data/" target="_blank" rel="external">Stanford Large Network Dataset Collection</a></li>
<li><a href="https://github.com/lw-lin/CoolplaySpark" target="_blank" rel="external">酷玩 Spark</a></li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzA3OTAxMDQzNQ==&amp;mid=2650607874&amp;idx=1&amp;sn=3626d0d3d16be6236cfc9922eb0cd295&amp;scene=23&amp;srcid=0524320Jr36VjpcvuRJsenht#rd" target="_blank" rel="external">Spark知识体系完整解读</a></li>
<li><a href="https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html" target="_blank" rel="external">Deep Dive into Spark SQL’s Catalyst Optimizer</a></li>
<li><a href="http://www.csdn.net/article/1970-01-01/2824823" target="_blank" rel="external">GC调优在Spark应用中的实践</a></li>
<li><a href="https://cs.stanford.edu/~matei/" target="_blank" rel="external">Matei Zaharia</a></li>
<li><a href="http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf" target="_blank" rel="external">PDF Spark SQL: Relational Data Processing in Spark</a></li>
</ol>

      
    </div>
    <footer>
      
        
        
      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>





<nav id="pagination">
  
    <a href="/page/7/" class="alignleft prev">Prev</a>
  
  
  <div class="clearfix"></div>
</nav></div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Search">
    <input type="hidden" name="q" value="site:yoursite.com">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">Categories</h3>
  <ul class="entry">
  
    <li><a href="/categories/Algorithms/">Algorithms</a><small>1</small></li>
  
    <li><a href="/categories/Design/">Design</a><small>1</small></li>
  
    <li><a href="/categories/Distributed/">Distributed</a><small>9</small></li>
  
    <li><a href="/categories/Language/">Language</a><small>4</small></li>
  
    <li><a href="/categories/Machine-Learning/">Machine Learning</a><small>1</small></li>
  
    <li><a href="/categories/OP/">OP</a><small>1</small></li>
  
    <li><a href="/categories/Storage/">Storage</a><small>2</small></li>
  
    <li><a href="/categories/live/">live</a><small>3</small></li>
  
    <li><a href="/categories/machine-learning/">machine_learning</a><small>1</small></li>
  
  </ul>
</div>


  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/tags/Algorithms/">Algorithms</a><small>1</small></li>
  
    <li><a href="/tags/Alluxio/">Alluxio</a><small>1</small></li>
  
    <li><a href="/tags/Clojure/">Clojure</a><small>1</small></li>
  
    <li><a href="/tags/Design/">Design</a><small>1</small></li>
  
    <li><a href="/tags/Hadoop/">Hadoop</a><small>5</small></li>
  
    <li><a href="/tags/JVM/">JVM</a><small>1</small></li>
  
    <li><a href="/tags/Java/">Java</a><small>1</small></li>
  
    <li><a href="/tags/Linux/">Linux</a><small>1</small></li>
  
    <li><a href="/tags/Machine-Learning/">Machine Learning</a><small>1</small></li>
  
    <li><a href="/tags/MySQL/">MySQL</a><small>1</small></li>
  
    <li><a href="/tags/RocksDB/">RocksDB</a><small>1</small></li>
  
    <li><a href="/tags/Scala/">Scala</a><small>1</small></li>
  
    <li><a href="/tags/Streaming-Process/">Streaming Process</a><small>1</small></li>
  
    <li><a href="/tags/TensorFlow/">TensorFlow</a><small>1</small></li>
  
    <li><a href="/tags/ZooKeeper/">ZooKeeper</a><small>1</small></li>
  
    <li><a href="/tags/coffee/">coffee</a><small>1</small></li>
  
    <li><a href="/tags/fitness/">fitness</a><small>1</small></li>
  
    <li><a href="/tags/game/">game</a><small>1</small></li>
  
    <li><a href="/tags/spark/">spark</a><small>1</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2016 Darion Yaphet
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>
