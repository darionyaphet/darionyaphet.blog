<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>spark | darion.johannes.yaphet</title>
  <meta name="author" content="Darion Yaphet">
  
  <meta name="description" content="SparkApache Spark is a fast and general-purpose cluster computing system. 
It provides high-level APIs in Java, Scala, Python and R, and an optimized ">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <meta property="og:title" content="spark"/>
  <meta property="og:site_name" content="darion.johannes.yaphet"/>

  
    <meta property="og:image" content="undefined"/>
  

  <link href="/favicon.png" rel="icon">
  <link rel="alternate" href="/atom.xml" title="darion.johannes.yaphet" type="application/atom+xml">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->
  

</head>


<body>
  <header id="header" class="inner"><div class="alignleft">
  <h1><a href="/">darion.johannes.yaphet</a></h1>
  <h2><a href="/">long is the way and hard  that out of Hell leads up to light</a></h2>
</div>
<nav id="main-nav" class="alignright">
  <ul>
    
      <li><a href="/">Home</a></li>
    
      <li><a href="/archives">Archives</a></li>
    
  </ul>
  <div class="clearfix"></div>
</nav>
<div class="clearfix"></div>
</header>
  <div id="content" class="inner">
    <div id="main-col" class="alignleft"><div id="wrapper"><article class="post">
  
  <div class="post-content">
    <header>
      
        <div class="icon"></div>
        <time datetime="2015-12-31T16:00:00.000Z"><a href="/2016/01/01/distribution/spark/">2016-01-01</a></time>
      
      
  
    <h1 class="title">spark</h1>
  

    </header>
    <div class="entry">
      
        <h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><p>Apache Spark is a fast and general-purpose cluster computing system. </p>
<p>It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. </p>
<p>It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.</p>
<p>Spark’s primary abstraction is a distributed collection of items called a Resilient Distributed Dataset (RDD). RDDs can be created from Hadoop InputFormats or by transforming other RDDs. </p>
<p>RDDs have actions, which return values, and transformations, which return pointers to new RDDs.</p>
<p><img src="../resource/distribution/spark/architecture.jpg" alt="architecture"></p>
<h3 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h3><p>One of the strongest features of Spark is its shell. </p>
<p>The Spark-Shell allows users to type and execute commands in a Unix-Terminal-like fashion. </p>
<ol>
<li><p>To adjust the amount of memory that Spark may use for executing queries you have to set the following environment prior to starting the shell <code>export SPARK_MEM =1 g</code></p>
</li>
<li><p>To  controls the number of worker threads that Spark uses <code>export SPARK_WORKER_INSTANCES =4</code> If you run Spark in local mode you can also set the number of worker threads in one setting as follows: <code>export MASTER = local [32]</code></p>
</li>
</ol>
<hr>
<h3 id="RDD-API"><a href="#RDD-API" class="headerlink" title="RDD API"></a>RDD API</h3><p>RDD is short for Resilient Distributed Dataset. RDDs are the workhorse of the Spark<br>system. As a user, one can consider a RDD as a handle for a collection of individual data partitions which are the result of some computation.</p>
<p>RDDs have <strong>actions</strong>, which return values, and <strong>transformations</strong>, which return pointers to new RDDs.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">λ ~/service/spark-1.6.0/ bin/spark-shell</span><br><span class="line">log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).</span><br><span class="line">log4j:WARN Please initialize the log4j system properly.</span><br><span class="line">log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span><br><span class="line">Using Spark&apos;s repl log4j profile: org/apache/spark/log4j-defaults-repl.properties</span><br><span class="line">To adjust logging level use sc.setLogLevel(&quot;INFO&quot;)</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)</span><br><span class="line">Type in expressions to have them evaluated.</span><br><span class="line">Type :help for more information.</span><br><span class="line">16/01/24 21:53:16 WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.1.142 instead (on interface en0)</span><br><span class="line">16/01/24 21:53:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address</span><br><span class="line">Spark context available as sc.</span><br><span class="line">SQL context available as sqlContext.</span><br><span class="line"></span><br><span class="line">scala&gt; val list = sc.parallelize(List(1,2,3,4,5,6), 2)</span><br><span class="line">list: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:27</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h3><p>Spark SQL is a Spark module for structured data processing. </p>
<p>It provides a programming abstraction called DataFrames and can also act as distributed SQL query engine.</p>
<ol>
<li>DataFrame: a distributed collection of data organized into named columns.</li>
</ol>
<hr>
<h3 id="Streaming"><a href="#Streaming" class="headerlink" title="Streaming"></a>Streaming</h3><p>Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput, fault-tolerant stream processing of live data streams. Data can be ingested from many sources like Kafka, Flume, Twitter, ZeroMQ, Kinesis, or TCP sockets, and can be processed using complex algorithms expressed with high-level functions like map, reduce, join and window. Finally, processed data can be pushed out to filesystems, databases, and live dashboards. In fact, you can apply Spark’s machine learning and graph processing algorithms on data streams.</p>
<p>Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data.</p>
<p>Internally, a DStream is represented as a sequence of RDDs.</p>
<p><img src="../resource/distribution/spark/streaming-dstream-window.png" alt="streaming-dstream-window"></p>
<hr>
<h3 id="GraphX"><a href="#GraphX" class="headerlink" title="GraphX"></a>GraphX</h3><hr>
<h3 id="MLlib"><a href="#MLlib" class="headerlink" title="MLlib"></a>MLlib</h3><hr>
<h3 id="R"><a href="#R" class="headerlink" title="R"></a>R</h3><p>SparkR provides a distributed data frame implementation that supports operations like selection, filtering, aggregation etc.</p>
<p>SparkR also supports distributed machine learning using MLlib.</p>
<hr>
<h3 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h3><p>There are two deploy modes that can be used to launch Spark applications on YARN.</p>
<p><strong>In cluster mode</strong>, the Spark driver runs inside an application master process which is managed by YARN on the cluster, and the client can go away after initiating the application. </p>
<p><strong>In client mode</strong>, the driver runs in the client process, and the application master is only used for requesting resources from YARN.</p>
<p>In YARN mode the ResourceManager’s address is picked up from the Hadoop configuration. Thus, the <code>--master</code> parameter is yarn.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class path.to.your.Class --master yarn --deploy-mode cluster [options] &lt;app jar&gt; [app options]</span><br></pre></td></tr></table></figure>
<p><strong>Config</strong></p>
<p><strong>Application Properties</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.app.name</td>
<td>The name of your application.</td>
</tr>
<tr>
<td>spark.driver.cores</td>
<td>Number of cores to use for the driver process (only cluster mode).</td>
</tr>
<tr>
<td>spark.driver.maxResultSize</td>
<td>Limit of total size of serialized results of all partitions for each Spark action.</td>
</tr>
<tr>
<td>spark.driver.memory</td>
<td>Memory to use for the driver process</td>
</tr>
<tr>
<td>spark.executor.memory</td>
<td>Memory to use per executor process</td>
</tr>
<tr>
<td>spark.extraListeners</td>
<td></td>
</tr>
<tr>
<td>spark.local.dir</td>
<td></td>
</tr>
<tr>
<td>spark.logConf</td>
<td></td>
</tr>
<tr>
<td>spark.master</td>
<td>The cluster manager to connect </td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.yarn.am.memory</td>
<td>Amount of memory to use for the YARN Application Master in client mode</td>
</tr>
<tr>
<td>spark.driver.cores</td>
<td>Number of cores used by the driver in YARN cluster mode.</td>
</tr>
<tr>
<td>spark.yarn.am.cores</td>
<td>Number of cores to use for the YARN Application Master in client mode.</td>
</tr>
<tr>
<td>spark.yarn.am.waitTime</td>
<td>In cluster mode, time for the YARN Application Master to wait for the SparkContext to be initialized. In client mode, time for the YARN Application Master to wait for the driver to connect to it.</td>
</tr>
<tr>
<td>spark.yarn.submit.file.replication</td>
<td>HDFS replication level for the files uploaded into HDFS. </td>
</tr>
<tr>
<td>spark.yarn.preserve.staging.files</td>
<td>Set to true to preserve the staged files at the end of the job.</td>
</tr>
<tr>
<td>spark.yarn.scheduler.heartbeat.interval-ms</td>
<td>Spark application master heartbeats into the YARN ResourceManager interval.</td>
</tr>
<tr>
<td>spark.yarn.scheduler.initial-allocation.interval</td>
<td>The initial interval in which the Spark application master eagerly heartbeats to the YARN ResourceManager when there are pending container allocation requests. </td>
</tr>
<tr>
<td>spark.yarn.max.executor.failures</td>
<td>The maximum number of executor failures before failing the application.</td>
</tr>
<tr>
<td>spark.yarn.historyServer.address</td>
<td>The address of the Spark history server</td>
</tr>
<tr>
<td>spark.yarn.dist.archives</td>
<td>Comma separated list of archives to be extracted into the working directory of each executor.</td>
</tr>
<tr>
<td>spark.yarn.dist.files</td>
<td>Comma-separated list of files to be placed in the working directory of each executor.</td>
</tr>
<tr>
<td>spark.executor.instances</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.executor.memoryOverhead</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.driver.memoryOverhead</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.memoryOverhead</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.port</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.queue</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.jar</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.access.namenodes</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.appMasterEnv.[EnvironmentVariableName]</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.containerLauncherMaxThreads</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.extraJavaOptions</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.extraLibraryPath</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.maxAppAttempts</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.attemptFailuresValidityInterval</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.submit.waitAppCompletion</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.am.nodeLabelExpression</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.executor.nodeLabelExpression</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.tags</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.keytab</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.principal</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.config.gatewayPath</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.config.replacementPath</td>
<td></td>
</tr>
<tr>
<td>spark.yarn.security.tokens.${service}.enabled</td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Shuffle Behavior</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.reducer.maxSizeInFlight</td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Spark UI</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Compression and Serialization</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Memory Management</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Execution Behavior</strong></p>
<table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
<p><img src="../resource/distribution/spark/spark_on_yarn.jpg" alt="spark_on_yarn"></p>
<hr>
<h3 id="PySpark"><a href="#PySpark" class="headerlink" title="PySpark"></a>PySpark</h3><h3 id="Config"><a href="#Config" class="headerlink" title="Config"></a>Config</h3><table>
<thead>
<tr>
<th>Property Name</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.app.name</td>
<td>The name of your application. </td>
</tr>
<tr>
<td>spark.driver.cores</td>
<td>Number of cores to use for the driver process, only in cluster mode.</td>
</tr>
</tbody>
</table>
<hr>
<h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>Spark applications run as independent sets of processes on a cluster,coordinated by the SparkContext (<code>driver program</code>).</p>
<p>SparkContext can connect to several types of cluster managers.</p>
<p><img src="../resource/distribution/spark/cluster-overview.png" alt="architecture"></p>
<table>
<thead>
<tr>
<th>Term</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Application</td>
<td>User program built on Spark.</td>
</tr>
<tr>
<td>Driver program</td>
<td>The process running the main() function of the application and creating the SparkContext</td>
</tr>
<tr>
<td>Cluster manager</td>
<td>An external service for acquiring resources on the cluster</td>
</tr>
<tr>
<td>Deploy mode</td>
<td>Distinguishes where the driver process runs.</td>
</tr>
<tr>
<td>Worker node</td>
<td>Any node that can run application code in the cluster</td>
</tr>
<tr>
<td>Executor</td>
<td>A process that runs tasks and keeps data in memory or disk storage.</td>
</tr>
<tr>
<td>Task</td>
<td>A unit of work that will be sent to one executor</td>
</tr>
<tr>
<td>Job</td>
<td>A parallel computation consisting of multiple tasks</td>
</tr>
<tr>
<td>Stage</td>
<td>Each job gets divided into smaller sets of tasks</td>
</tr>
</tbody>
</table>
<p>Reference:</p>
<ol>
<li><a href="http://www.infoq.com/cn/articles/2015-Review-Spark" target="_blank" rel="external">解读2015之Spark篇：新生态系统的形成</a></li>
<li><a href="http://www.infoq.com/cn/presentations/gc-tuning-of-spark-application?utm_campaign=rightbar_v2&amp;utm_source=infoq&amp;utm_medium=presentations_link&amp;utm_content=link_text" target="_blank" rel="external">Spark应用的GC调优</a></li>
<li><a href="http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html" target="_blank" rel="external">The RDD API By Example</a></li>
<li><a href="https://databricks.com/blog" target="_blank" rel="external">Databricks Blog</a></li>
<li><a href="https://databricks.gitbooks.io/databricks-spark-reference-applications/content/index.html" target="_blank" rel="external">databricks.gitbooks.io</a></li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzA5MTc0NTMwNQ==&amp;mid=402777570&amp;idx=1&amp;sn=b06881f5d374cc181784cd8ba31893ad&amp;scene=23&amp;srcid=0318eIjnKdNTXNnQxbXTVpyp#rd" target="_blank" rel="external">一个SparkSQL作业的一生</a></li>
<li><a href="http://mp.weixin.qq.com/s?__biz=MzA5MTc0NTMwNQ==&amp;mid=402803628&amp;idx=1&amp;sn=bd72f7e43ddefb0946121778ac161ab5&amp;scene=23&amp;srcid=0318z3Rle2LCaZomw5S45p9D#rd" target="_blank" rel="external">Spark生态顶级项目汇总</a></li>
</ol>

      
    </div>
    <footer>
      
        
  
  <div class="categories">
    <a href="/categories/dis/">dis</a>
  </div>

        
  
  <div class="tags">
    <a href="/tags/spark/">spark</a>
  </div>

        
  <div class="addthis addthis_toolbox addthis_default_style">
    
      <a class="addthis_button_facebook_like" fb:like:layout="button_count"></a>
    
    
      <a class="addthis_button_tweet"></a>
    
    
      <a class="addthis_button_google_plusone" g:plusone:size="medium"></a>
    
    
      <a class="addthis_button_pinterest_pinit" pi:pinit:layout="horizontal"></a>
    
    <a class="addthis_counter addthis_pill_style"></a>
  </div>
  <script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js"></script>

      
      <div class="clearfix"></div>
    </footer>
  </div>
</article>


<section id="comment">
  <h1 class="title">Kommentare</h1>

  
      <div id="fb-root"></div>
<script>
  (function(d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=123456789012345";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));
</script>

<div class="fb-comments" data-href="http://yoursite.com/2016/01/01/distribution/spark/index.html" data-num-posts="5" data-width="840" data-colorscheme="light"></div>
      
  
</section>

</div></div>
    <aside id="sidebar" class="alignright">
  <div class="search">
  <form action="//google.com/search" method="get" accept-charset="utf-8">
    <input type="search" name="q" results="0" placeholder="Suche">
    <input type="hidden" name="q" value="site:yoursite.com">
  </form>
</div>

  
<div class="widget tag">
  <h3 class="title">Kategorien</h3>
  <ul class="entry">
  
    <li><a href="/categories/dis/">dis</a><small>1</small></li>
  
  </ul>
</div>


  
<div class="widget tag">
  <h3 class="title">Tags</h3>
  <ul class="entry">
  
    <li><a href="/tags/spark/">spark</a><small>1</small></li>
  
  </ul>
</div>

</aside>
    <div class="clearfix"></div>
  </div>
  <footer id="footer" class="inner"><div class="alignleft">
  
  &copy; 2016 Darion Yaphet
  
</div>
<div class="clearfix"></div></footer>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>




<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>

</body>
</html>
